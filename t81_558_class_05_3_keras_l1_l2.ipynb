{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fouYPZo2r6_P"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_05_3_keras_l1_l2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nm6jUdLr6_R"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 5: Regularization and Dropout**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6omDsXzbr6_R"
   },
   "source": [
    "# Module 5 Material\n",
    "\n",
    "* Part 5.1: Part 5.1: Introduction to Regularization: Ridge and Lasso [[Video]](https://www.youtube.com/watch?v=jfgRtCYjoBs&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_05_1_reg_ridge_lasso.ipynb)\n",
    "* Part 5.2: Using K-Fold Cross Validation with PyTorch [[Video]](https://www.youtube.com/watch?v=maiQf8ray_s&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_05_2_kfold.ipynb)\n",
    "* **Part 5.3: Using L1 and L2 Regularization with PyTorch to Decrease Overfitting** [[Video]](https://www.youtube.com/watch?v=JEWzWv1fBFQ&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_05_3_keras_l1_l2.ipynb)\n",
    "* Part 5.4: Drop Out for PyTorch to Decrease Overfitting [[Video]](https://www.youtube.com/watch?v=bRyOi0L6Rs8&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_05_4_dropout.ipynb)\n",
    "* Part 5.5: Benchmarking PyTorch Deep Learning Regularization Techniques [[Video]](https://www.youtube.com/watch?v=1NLBwPumUAs&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_05_5_bootstrap.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuSZFOpor6_S"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JT3DWsevr6_S",
    "outputId": "2b9c8b7a-768b-47c3-81c7-10342cd9a4a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVJm05pTr6_T"
   },
   "source": [
    "# Part 5.3: L1 and L2 Regularization to Decrease Overfitting\n",
    "\n",
    "L1 and L2 regularization are two common regularization techniques that can reduce the effects of overfitting [[Cite:ng2004feature]](http://cseweb.ucsd.edu/~elkan/254spring05/Hammon.pdf). These algorithms can either work with an objective function or as a part of the backpropagation algorithm. In both cases, the regularization algorithm is attached to the training algorithm by adding an objective.  \n",
    "\n",
    "These algorithms work by adding a weight penalty to the neural network training. This penalty encourages the neural network to keep the weights to small values. Both L1 and L2 calculate this penalty differently. You can add this penalty calculation to the calculated gradients for gradient-descent-based algorithms, such as backpropagation. The penalty is negatively combined with the objective score for objective-function-based training, such as simulated annealing.\n",
    "\n",
    "Both L1 and L2 work differently in that they penalize the size of the weight. L2 will force the weights into a pattern similar to a Gaussian distribution; the L1 will force the weights into a pattern similar to a Laplace distribution, as demonstrated in Figure 5.L1L2.\n",
    "\n",
    "**Figure 5.L1L2: L1 vs L2**\n",
    "![L1 vs L2](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_9_l1_l2.png \"L1 vs L2\")\n",
    "\n",
    "As you can see, L1 algorithm is more tolerant of weights further from 0, whereas the L2 algorithm is less tolerant. We will highlight other important differences between L1 and L2 in the following sections. \n",
    "\n",
    "We begin by accessing CUDA if a GPU is available; otherwise, we will use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PcRXtHkK8vMj",
    "outputId": "4d7af928-21f2-48f8-84da-4d10b2c0d1b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mphCKGe7YVxd"
   },
   "source": [
    "Next, we define the functions that calculate L1 and L2 normalization loss. We calculate the loss across all weights biases, which is not a universal practice. Some implementations may calculate the loss over just the weights, excluding biases. Other implementations may choose a specific layer. For simplicity, we sum over all parameters, including weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fQ-DtA-ishmf"
   },
   "outputs": [],
   "source": [
    "def add_l2_norm_loss(model, l2_lambda = 0.001):\n",
    "  l2_norm = sum(p.pow(2.0).sum()\n",
    "    for p in model.parameters())\n",
    "  return l2_lambda * l2_norm\n",
    "  \n",
    "def add_l1_norm_loss(model, l1_lambda = 0.001):\n",
    "  l1_norm = sum(p.abs().sum()\n",
    "    for p in model.parameters())\n",
    "  return l1_lambda * l1_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0SFN2EJy7dGL"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import copy\n",
    "\n",
    "class EarlyStopping():\n",
    "  def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
    "    self.patience = patience\n",
    "    self.min_delta = min_delta\n",
    "    self.restore_best_weights = restore_best_weights\n",
    "    self.best_model = None\n",
    "    self.best_loss = None\n",
    "    self.counter = 0\n",
    "    self.status = \"\"\n",
    "    \n",
    "  def __call__(self, model, val_loss):\n",
    "    if self.best_loss == None:\n",
    "      self.best_loss = val_loss\n",
    "      self.best_model = copy.deepcopy(model)\n",
    "    elif self.best_loss - val_loss > self.min_delta:\n",
    "      self.best_loss = val_loss\n",
    "      self.counter = 0\n",
    "      self.best_model.load_state_dict(model.state_dict())\n",
    "    elif self.best_loss - val_loss < self.min_delta:\n",
    "      self.counter += 1\n",
    "      if self.counter >= self.patience:\n",
    "        self.status = f\"Stopped on {self.counter}\"\n",
    "        if self.restore_best_weights:\n",
    "          model.load_state_dict(self.best_model.state_dict())\n",
    "        return True\n",
    "    self.status = f\"{self.counter}/{self.patience}\"\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lgOSmzBUr6_T"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Read the data set\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Generate dummies for job\n",
    "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1)\n",
    "df.drop('job', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for area\n",
    "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n",
    "df.drop('area', axis=1, inplace=True)\n",
    "\n",
    "# Missing values for income\n",
    "med = df['income'].median()\n",
    "df['income'] = df['income'].fillna(med)\n",
    "\n",
    "# Standardize ranges\n",
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['age'] = zscore(df['age'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x_columns = df.columns.drop('product').drop('id')\n",
    "x = df[x_columns].values\n",
    "dummies = pd.get_dummies(df['product']) # Classification\n",
    "products = dummies.columns\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQ1iemxCr6_U"
   },
   "source": [
    "We now create a PyTorch network with L1 regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yk2h2wplr6_U",
    "outputId": "89ba37d3-f5c1-44f1-cf7c-49e70e4a6605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, tloss: 1.752314805984497, vloss: 1.685029, EStop:[0/5]: 100%|██████████| 100/100 [00:02<00:00, 44.64it/s]\n",
      "Epoch: 2, tloss: 1.5662773847579956, vloss: 1.681054, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 91.07it/s]\n",
      "Epoch: 3, tloss: 1.6772552728652954, vloss: 1.679820, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 74.93it/s]\n",
      "Epoch: 4, tloss: 1.5350453853607178, vloss: 1.672797, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 81.93it/s]\n",
      "Epoch: 5, tloss: 1.6077457666397095, vloss: 1.650317, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 96.04it/s]\n",
      "Epoch: 6, tloss: 1.5640835762023926, vloss: 1.577708, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 92.39it/s]\n",
      "Epoch: 7, tloss: 1.4834266901016235, vloss: 1.537069, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 76.59it/s]\n",
      "Epoch: 8, tloss: 1.6142799854278564, vloss: 1.525642, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 74.16it/s]\n",
      "Epoch: 9, tloss: 1.7124179601669312, vloss: 1.562716, EStop:[1/5]: 100%|██████████| 100/100 [00:01<00:00, 78.83it/s]\n",
      "Epoch: 10, tloss: 1.4278934001922607, vloss: 1.525052, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 75.23it/s]\n",
      "Epoch: 11, tloss: 1.4176121950149536, vloss: 1.532820, EStop:[1/5]: 100%|██████████| 100/100 [00:01<00:00, 75.01it/s]\n",
      "Epoch: 12, tloss: 1.5345462560653687, vloss: 1.550239, EStop:[2/5]: 100%|██████████| 100/100 [00:01<00:00, 72.15it/s]\n",
      "Epoch: 13, tloss: 1.5723055601119995, vloss: 1.523720, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 75.51it/s]\n",
      "Epoch: 14, tloss: 1.4119701385498047, vloss: 1.519467, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 86.88it/s]\n",
      "Epoch: 15, tloss: 1.6144102811813354, vloss: 1.513052, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 91.25it/s]\n",
      "Epoch: 16, tloss: 1.455672025680542, vloss: 1.523988, EStop:[1/5]: 100%|██████████| 100/100 [00:01<00:00, 81.91it/s]\n",
      "Epoch: 17, tloss: 1.6431041955947876, vloss: 1.543326, EStop:[2/5]: 100%|██████████| 100/100 [00:01<00:00, 80.39it/s]\n",
      "Epoch: 18, tloss: 1.5855052471160889, vloss: 1.525025, EStop:[3/5]: 100%|██████████| 100/100 [00:01<00:00, 78.79it/s]\n",
      "Epoch: 19, tloss: 1.4191670417785645, vloss: 1.520088, EStop:[4/5]: 100%|██████████| 100/100 [00:00<00:00, 100.84it/s]\n",
      "Epoch: 20, tloss: 1.5774295330047607, vloss: 1.540759, EStop:[Stopped on 5]: 100%|██████████| 100/100 [00:00<00:00, 133.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold score (accuracy): 0.6575\n",
      "Fold #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, tloss: 1.736241340637207, vloss: 1.693863, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 130.98it/s]\n",
      "Epoch: 2, tloss: 1.7167763710021973, vloss: 1.693886, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 126.23it/s]\n",
      "Epoch: 3, tloss: 1.5063915252685547, vloss: 1.700532, EStop:[2/5]: 100%|██████████| 100/100 [00:00<00:00, 134.01it/s]\n",
      "Epoch: 4, tloss: 1.5894280672073364, vloss: 1.647491, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 125.84it/s]\n",
      "Epoch: 5, tloss: 1.700634479522705, vloss: 1.530557, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 134.82it/s]\n",
      "Epoch: 6, tloss: 1.676051139831543, vloss: 1.520396, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 135.74it/s]\n",
      "Epoch: 7, tloss: 1.474263072013855, vloss: 1.515694, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 134.37it/s]\n",
      "Epoch: 8, tloss: 1.4682118892669678, vloss: 1.537373, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 127.53it/s]\n",
      "Epoch: 9, tloss: 1.4142489433288574, vloss: 1.514477, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 123.34it/s]\n",
      "Epoch: 10, tloss: 1.4390534162521362, vloss: 1.506933, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 127.97it/s]\n",
      "Epoch: 11, tloss: 1.5790696144104004, vloss: 1.560892, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 123.12it/s]\n",
      "Epoch: 12, tloss: 1.5837303400039673, vloss: 1.535788, EStop:[2/5]: 100%|██████████| 100/100 [00:00<00:00, 128.75it/s]\n",
      "Epoch: 13, tloss: 1.5957074165344238, vloss: 1.506520, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 130.66it/s]\n",
      "Epoch: 14, tloss: 1.5344616174697876, vloss: 1.507174, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 128.11it/s]\n",
      "Epoch: 15, tloss: 1.4189645051956177, vloss: 1.511050, EStop:[2/5]: 100%|██████████| 100/100 [00:00<00:00, 125.94it/s]\n",
      "Epoch: 16, tloss: 1.4432117938995361, vloss: 1.507410, EStop:[3/5]: 100%|██████████| 100/100 [00:00<00:00, 126.29it/s]\n",
      "Epoch: 17, tloss: 1.5635087490081787, vloss: 1.505614, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 128.02it/s]\n",
      "Epoch: 18, tloss: 1.5192915201187134, vloss: 1.505543, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 132.66it/s]\n",
      "Epoch: 19, tloss: 1.5762494802474976, vloss: 1.528684, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 128.72it/s]\n",
      "Epoch: 20, tloss: 1.5142327547073364, vloss: 1.502248, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 127.07it/s]\n",
      "Epoch: 21, tloss: 1.4752541780471802, vloss: 1.506535, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 129.01it/s]\n",
      "Epoch: 22, tloss: 1.6537405252456665, vloss: 1.502437, EStop:[2/5]: 100%|██████████| 100/100 [00:00<00:00, 127.02it/s]\n",
      "Epoch: 23, tloss: 1.4610809087753296, vloss: 1.509766, EStop:[3/5]: 100%|██████████| 100/100 [00:00<00:00, 123.73it/s]\n",
      "Epoch: 24, tloss: 1.5780587196350098, vloss: 1.509369, EStop:[4/5]: 100%|██████████| 100/100 [00:00<00:00, 128.46it/s]\n",
      "Epoch: 25, tloss: 1.5173640251159668, vloss: 1.506329, EStop:[Stopped on 5]: 100%|██████████| 100/100 [00:00<00:00, 128.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold score (accuracy): 0.6675\n",
      "Fold #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, tloss: 1.6426591873168945, vloss: 1.664748, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 128.59it/s]\n",
      "Epoch: 2, tloss: 1.6889688968658447, vloss: 1.680957, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 129.49it/s]\n",
      "Epoch: 3, tloss: 1.6923986673355103, vloss: 1.681552, EStop:[2/5]: 100%|██████████| 100/100 [00:00<00:00, 127.29it/s]\n",
      "Epoch: 4, tloss: 1.7985219955444336, vloss: 1.681252, EStop:[3/5]: 100%|██████████| 100/100 [00:00<00:00, 121.76it/s]\n",
      "Epoch: 5, tloss: 1.7435836791992188, vloss: 1.681841, EStop:[4/5]: 100%|██████████| 100/100 [00:00<00:00, 129.93it/s]\n",
      "Epoch: 6, tloss: 1.621694564819336, vloss: 1.681574, EStop:[Stopped on 5]: 100%|██████████| 100/100 [00:00<00:00, 129.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold score (accuracy): 0.485\n",
      "Fold #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, tloss: 1.6960283517837524, vloss: 1.665394, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 125.49it/s]\n",
      "Epoch: 2, tloss: 1.7043899297714233, vloss: 1.663143, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 130.55it/s]\n",
      "Epoch: 3, tloss: 1.6685619354248047, vloss: 1.660332, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 125.95it/s]\n",
      "Epoch: 4, tloss: 1.6569678783416748, vloss: 1.644966, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 124.59it/s]\n",
      "Epoch: 5, tloss: 1.5483955144882202, vloss: 1.585966, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 129.24it/s]\n",
      "Epoch: 6, tloss: 1.6016675233840942, vloss: 1.551459, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 123.49it/s]\n",
      "Epoch: 7, tloss: 1.5955419540405273, vloss: 1.572304, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 120.14it/s]\n",
      "Epoch: 8, tloss: 1.6457254886627197, vloss: 1.549885, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 126.09it/s]\n",
      "Epoch: 9, tloss: 1.4217344522476196, vloss: 1.542323, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 126.82it/s]\n",
      "Epoch: 10, tloss: 1.5348925590515137, vloss: 1.534470, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 134.04it/s]\n",
      "Epoch: 11, tloss: 1.272716760635376, vloss: 1.550946, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 118.64it/s]\n",
      "Epoch: 12, tloss: 1.5821090936660767, vloss: 1.551148, EStop:[2/5]: 100%|██████████| 100/100 [00:00<00:00, 125.42it/s]\n",
      "Epoch: 13, tloss: 1.5972793102264404, vloss: 1.547873, EStop:[3/5]: 100%|██████████| 100/100 [00:00<00:00, 124.25it/s]\n",
      "Epoch: 14, tloss: 1.5177441835403442, vloss: 1.548539, EStop:[4/5]: 100%|██████████| 100/100 [00:00<00:00, 125.87it/s]\n",
      "Epoch: 15, tloss: 1.5102111101150513, vloss: 1.549222, EStop:[Stopped on 5]: 100%|██████████| 100/100 [00:00<00:00, 127.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold score (accuracy): 0.63\n",
      "Fold #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1, tloss: 1.566257357597351, vloss: 1.688106, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 125.82it/s]\n",
      "Epoch: 2, tloss: 1.8711503744125366, vloss: 1.689323, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 132.07it/s]\n",
      "Epoch: 3, tloss: 1.6826667785644531, vloss: 1.689814, EStop:[2/5]: 100%|██████████| 100/100 [00:00<00:00, 126.14it/s]\n",
      "Epoch: 4, tloss: 1.92525315284729, vloss: 1.689516, EStop:[3/5]: 100%|██████████| 100/100 [00:00<00:00, 129.52it/s]\n",
      "Epoch: 5, tloss: 1.551872730255127, vloss: 1.689397, EStop:[4/5]: 100%|██████████| 100/100 [00:00<00:00, 124.12it/s]\n",
      "Epoch: 6, tloss: 1.801450252532959, vloss: 1.689434, EStop:[Stopped on 5]: 100%|██████████| 100/100 [00:00<00:00, 128.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold score (accuracy): 0.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "EPOCHS=500\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Define the PyTorch Neural Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_count, out_count):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_count, 50)\n",
    "        self.fc2 = nn.Linear(50, 25)\n",
    "        self.fc3 = nn.Linear(25, out_count)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.softmax(self.fc3(x))\n",
    "\n",
    "# Cross-Validate\n",
    "kf = KFold(5, shuffle=True, random_state=42) # Use for KFold classification\n",
    "oos_y_list = []\n",
    "oos_pred_list = []\n",
    "\n",
    "fold = 0\n",
    "for train, test in kf.split(x):\n",
    "    fold+=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "        \n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "\n",
    "    # Numpy to PyTorch\n",
    "    x_train = torch.Tensor(x_train).float()\n",
    "    y_train = torch.Tensor(y_train).float()\n",
    "\n",
    "    x_test = torch.Tensor(x_test).float().to(device)\n",
    "    y_test = torch.Tensor(y_test).float().to(device)\n",
    "\n",
    "    # Create datasets\n",
    "    dataset_train = TensorDataset(x_train, y_train)\n",
    "    dataloader_train = DataLoader(dataset_train,\\\n",
    "      batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    dataset_test = TensorDataset(x_test, y_test)\n",
    "    dataloader_test = DataLoader(dataset_test,\\\n",
    "      batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Train the network\n",
    "    model = Net(x.shape[1],len(products)).to(device)\n",
    "\n",
    "    # Define the loss function for classification\n",
    "    loss_fn = nn.CrossEntropyLoss()# cross entropy loss\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    es = EarlyStopping()\n",
    "\n",
    "    epoch = 0\n",
    "    done = False\n",
    "    while epoch<1000 and not done:\n",
    "      epoch += 1\n",
    "      steps = list(enumerate(dataloader_train))\n",
    "      pbar = tqdm.tqdm(steps)\n",
    "      model.train()\n",
    "      for i, (x_batch, y_batch) in pbar:\n",
    "        y_batch_pred = model(x_batch.to(device))\n",
    "        loss = loss_fn(y_batch_pred, y_batch.to(device))\n",
    "        loss += add_l1_norm_loss(model)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss, current = loss.item(), (i + 1)* len(x_batch)\n",
    "        if i == len(steps)-1:\n",
    "          model.eval()\n",
    "          pred = model(x_test)\n",
    "          vloss = loss_fn(pred, y_test)\n",
    "          if es(model,vloss): done = True\n",
    "          pbar.set_description(f\"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "        else:\n",
    "          pbar.set_description(f\"Epoch: {epoch}, tloss {loss:}\")\n",
    "    \n",
    "    pred = model(x_test)\n",
    "    \n",
    "    oos_y_list.append(y_test.cpu().detach())\n",
    "    oos_pred_list.append(pred.cpu().detach())    \n",
    "\n",
    "    # Measure this fold's RMSE\n",
    "    #score = np.sqrt(metrics.mean_squared_error(pred.cpu().detach(),y_test.cpu().detach()))\n",
    "    #print(f\"Fold score (RMSE): {score}\")\n",
    "\n",
    "    # Measure this fold's accuracy\n",
    "    y_compare = np.argmax(y_test.cpu().detach(),axis=1) # For accuracy calculation\n",
    "    pred = np.argmax(pred.cpu().detach(),axis=1) # For accuracy calculation\n",
    "    score = metrics.accuracy_score(y_compare, pred)\n",
    "    print(f\"Fold score (accuracy): {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RSGVQEHNr6_V",
    "outputId": "64800320-21b9-4382-b0e1-488ae4b809cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final OOS score (accuracy): 0.583\n"
     ]
    }
   ],
   "source": [
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y_list)\n",
    "oos_pred = np.concatenate(oos_pred_list)\n",
    "oos_y = np.argmax(oos_y,axis=1)\n",
    "oos_pred = np.argmax(oos_pred,axis=1)\n",
    "score = metrics.accuracy_score(oos_pred,oos_y)\n",
    "print(f\"Final OOS score (accuracy): {score}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of t81_558_class_05_3_keras_l1_l2.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
