{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0HWxcHskn4O"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_05_5_bootstrap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwoHV43Ukn4Q"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 5: Regularization and Dropout**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNcea-drkn4Q"
   },
   "source": [
    "# Module 5 Material\n",
    "\n",
    "* Part 5.1: Part 5.1: Introduction to Regularization: Ridge and Lasso [[Video]](https://www.youtube.com/watch?v=jfgRtCYjoBs&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_05_1_reg_ridge_lasso.ipynb)\n",
    "* Part 5.2: Using K-Fold Cross Validation with PyTorch [[Video]](https://www.youtube.com/watch?v=maiQf8ray_s&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_05_2_kfold.ipynb)\n",
    "* Part 5.3: Using L1 and L2 Regularization with PyTorch to Decrease Overfitting [[Video]](https://www.youtube.com/watch?v=JEWzWv1fBFQ&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_05_3_keras_l1_l2.ipynb)\n",
    "* Part 5.4: Drop Out for PyTorch to Decrease Overfitting [[Video]](https://www.youtube.com/watch?v=bRyOi0L6Rs8&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_05_4_dropout.ipynb)\n",
    "* **Part 5.5: Benchmarking PyTorch Deep Learning Regularization Techniques** [[Video]](https://www.youtube.com/watch?v=1NLBwPumUAs&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_05_5_bootstrap.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqyUhlDhkn4Q"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YsN_BrNTkn4R",
    "outputId": "f01c2eeb-f23e-4cf3-bf75-892843b36738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: using Google CoLab\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "try:\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "import io\n",
    "import copy\n",
    "\n",
    "# L2 Normlization\n",
    "def add_l2_norm_loss(model, l2_lambda = 0.001):\n",
    "  l2_norm = sum(p.pow(2.0).sum()\n",
    "    for p in model.parameters())\n",
    "  return l2_lambda * l2_norm\n",
    "  \n",
    "# L1 Normlization\n",
    "def add_l1_norm_loss(model, l1_lambda = 0.001):\n",
    "  l1_norm = sum(p.abs().sum()\n",
    "    for p in model.parameters())\n",
    "  return l1_lambda * l1_norm\n",
    "\n",
    "# Define class for early stopping. For more information, see module 3.4.\n",
    "class EarlyStopping():\n",
    "  def __init__(self, patience=5, min_delta=1e-4, restore_best_weights=True):\n",
    "    self.patience = patience\n",
    "    self.min_delta = min_delta\n",
    "    self.restore_best_weights = restore_best_weights\n",
    "    self.best_model = None\n",
    "    self.best_loss = None\n",
    "    self.counter = 0\n",
    "    self.status = \"\"\n",
    "    \n",
    "  def __call__(self, model, val_loss):\n",
    "    if self.best_loss == None:\n",
    "      self.best_loss = val_loss\n",
    "      self.best_model = copy.deepcopy(model)\n",
    "    elif self.best_loss - val_loss > self.min_delta:\n",
    "      self.best_loss = val_loss\n",
    "      self.counter = 0\n",
    "      self.best_model.load_state_dict(model.state_dict())\n",
    "    elif self.best_loss - val_loss < self.min_delta:\n",
    "      self.counter += 1\n",
    "      if self.counter >= self.patience:\n",
    "        self.status = f\"Stopped on {self.counter}\"\n",
    "        if self.restore_best_weights:\n",
    "          model.load_state_dict(self.best_model.state_dict())\n",
    "        return True\n",
    "    self.status = f\"{self.counter}/{self.patience}\"\n",
    "    return False\n",
    "\n",
    "# Make use of a GPU if one is available.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcplr2pVkn4S"
   },
   "source": [
    "# Part 5.5: Benchmarking Regularization Techniques\n",
    "\n",
    "Quite a few hyperparameters have been introduced so far.  Tweaking each of these values can have an effect on the score obtained by your neural networks.  Some of the hyperparameters seen so far include:\n",
    "\n",
    "* Number of layers in the neural network\n",
    "* How many neurons in each layer\n",
    "* What activation functions to use on each layer\n",
    "* Dropout percent on each layer\n",
    "* L1 and L2 values on each layer\n",
    "\n",
    "To try out each of these hyperparameters you will need to run train neural networks with multiple settings for each hyperparameter.  However, you may have noticed that neural networks often produce somewhat different results when trained multiple times.  This is because the neural networks start with random weights.  Because of this it is necessary to fit and evaluate a neural network times to ensure that one set of hyperparameters are actually better than another.  Bootstrapping can be an effective means of benchmarking (comparing) two sets of hyperparameters.  \n",
    "\n",
    "Bootstrapping is similar to cross-validation.  Both go through a number of cycles/folds providing validation and training sets.  However, bootstrapping can have an unlimited number of cycles.  Bootstrapping chooses a new train and validation split each cycle, with replacement.  The fact that each cycle is chosen with replacement means that, unlike cross validation, there will often be repeated rows selected between cycles.  If you run the bootstrap for enough cycles, there will be duplicate cycles.\n",
    "\n",
    "In this part we will use bootstrapping for hyperparameter benchmarking.  We will train a neural network for a specified number of splits (denoted by the SPLITS constant).  For these examples we use 100.  We will compare the average score at the end of the 100.  By the end of the cycles the mean score will have converged somewhat.  This ending score will be a much better basis of comparison than a single cross-validation.  Additionally, the average number of epochs will be tracked to give an idea of a possible optimal value.  Because the early stopping validation set is also used to evaluate the the neural network as well, it might be slightly inflated.  This is because we are both stopping and evaluating on the same sample.  However, we are using the scores only as relative measures to determine the superiority of one set of hyperparameters to another, so this slight inflation should not present too much of a problem.\n",
    "\n",
    "Because we are benchmarking, we will display the amount of time taken for each cycle.  The following function can be used to nicely format a time span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "k_xChO0Fkn4S"
   },
   "outputs": [],
   "source": [
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOBKcIUzkn4T"
   },
   "source": [
    "## Bootstrapping for Regression\n",
    "\n",
    "Regression bootstrapping uses the **ShuffleSplit** object to perform the splits.  This technique is similar to **KFold** for cross-validation; no balancing occurs.  We will attempt to predict the age column for the **jh-simple-dataset**; the following code loads this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yWKzWCRskn4T"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data set\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Generate dummies for job\n",
    "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1)\n",
    "df.drop('job', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for area\n",
    "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n",
    "df.drop('area', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for product\n",
    "df = pd.concat([df,pd.get_dummies(df['product'],prefix=\"product\")],axis=1)\n",
    "df.drop('product', axis=1, inplace=True)\n",
    "\n",
    "# Missing values for income\n",
    "med = df['income'].median()\n",
    "df['income'] = df['income'].fillna(med)\n",
    "\n",
    "# Standardize ranges\n",
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x_columns = df.columns.drop('age').drop('id')\n",
    "x = df[x_columns].values\n",
    "y = df['age'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mHk4XHLkn4T"
   },
   "source": [
    "The following code performs the bootstrap.  The architecture of the neural network can be adjusted to compare many different configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WbhxDX1rkn4U",
    "outputId": "c11715a0-c045-4536-c2a5-fae224ab631a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1: score=45.117290, mean score=45.117290, stdev=0.000000  epochs=6, mean epochs=6  time=0:00:04.31\n",
      "#2: score=44.898441, mean score=45.007866, stdev=0.109425  epochs=6, mean epochs=6  time=0:00:01.71\n",
      "#3: score=45.204094, mean score=45.073275, stdev=0.128605  epochs=6, mean epochs=6  time=0:00:01.69\n",
      "#4: score=45.553978, mean score=45.193451, stdev=0.236074  epochs=6, mean epochs=6  time=0:00:01.57\n",
      "#5: score=44.616253, mean score=45.078011, stdev=0.312874  epochs=6, mean epochs=6  time=0:00:01.56\n",
      "#6: score=44.611824, mean score=45.000313, stdev=0.334305  epochs=6, mean epochs=6  time=0:00:01.57\n",
      "#7: score=44.856827, mean score=44.979815, stdev=0.313553  epochs=6, mean epochs=6  time=0:00:01.57\n",
      "#8: score=45.120007, mean score=44.997339, stdev=0.296943  epochs=6, mean epochs=6  time=0:00:01.55\n",
      "#9: score=45.166359, mean score=45.016119, stdev=0.284955  epochs=6, mean epochs=6  time=0:00:01.57\n",
      "#10: score=44.665646, mean score=44.981072, stdev=0.290059  epochs=6, mean epochs=6  time=0:00:01.55\n",
      "#11: score=45.184566, mean score=44.999571, stdev=0.282680  epochs=6, mean epochs=6  time=0:00:01.54\n",
      "#12: score=45.272732, mean score=45.022335, stdev=0.280979  epochs=6, mean epochs=6  time=0:00:01.53\n",
      "#13: score=44.673649, mean score=44.995513, stdev=0.285498  epochs=6, mean epochs=6  time=0:00:01.54\n",
      "#14: score=45.317768, mean score=45.018531, stdev=0.287359  epochs=6, mean epochs=6  time=0:00:01.54\n",
      "#15: score=44.935898, mean score=45.013022, stdev=0.278379  epochs=6, mean epochs=6  time=0:00:01.54\n",
      "#16: score=44.418804, mean score=44.975883, stdev=0.305517  epochs=6, mean epochs=6  time=0:00:01.54\n",
      "#17: score=44.698601, mean score=44.959573, stdev=0.303491  epochs=6, mean epochs=6  time=0:00:01.54\n",
      "#18: score=45.139839, mean score=44.969588, stdev=0.297816  epochs=6, mean epochs=6  time=0:00:01.54\n",
      "#19: score=45.156837, mean score=44.979443, stdev=0.292873  epochs=6, mean epochs=6  time=0:00:01.54\n",
      "#20: score=45.162651, mean score=44.988603, stdev=0.288237  epochs=6, mean epochs=6  time=0:00:01.53\n",
      "#21: score=44.806194, mean score=44.979917, stdev=0.283960  epochs=6, mean epochs=6  time=0:00:01.53\n",
      "#22: score=45.190762, mean score=44.989501, stdev=0.280886  epochs=6, mean epochs=6  time=0:00:01.54\n",
      "#23: score=44.849804, mean score=44.983427, stdev=0.276185  epochs=6, mean epochs=6  time=0:00:01.56\n",
      "#24: score=45.000332, mean score=44.984131, stdev=0.270391  epochs=6, mean epochs=6  time=0:00:01.56\n",
      "#25: score=45.351074, mean score=44.998809, stdev=0.274513  epochs=6, mean epochs=6  time=0:00:01.56\n",
      "#26: score=44.670628, mean score=44.986187, stdev=0.276481  epochs=6, mean epochs=6  time=0:00:01.55\n",
      "#27: score=45.116459, mean score=44.991012, stdev=0.272426  epochs=6, mean epochs=6  time=0:00:01.55\n",
      "#28: score=44.953365, mean score=44.989667, stdev=0.267609  epochs=6, mean epochs=6  time=0:00:01.54\n",
      "#29: score=45.168850, mean score=44.995846, stdev=0.264979  epochs=6, mean epochs=6  time=0:00:01.55\n",
      "#30: score=44.558838, mean score=44.981279, stdev=0.272079  epochs=6, mean epochs=6  time=0:00:01.55\n",
      "#31: score=45.143826, mean score=44.986522, stdev=0.269191  epochs=6, mean epochs=6  time=0:00:01.55\n",
      "#32: score=44.666767, mean score=44.976530, stdev=0.270730  epochs=6, mean epochs=6  time=0:00:01.54\n",
      "#33: score=44.886467, mean score=44.973801, stdev=0.267043  epochs=6, mean epochs=6  time=0:00:01.55\n",
      "#34: score=45.065010, mean score=44.976484, stdev=0.263537  epochs=6, mean epochs=6  time=0:00:01.55\n",
      "#35: score=44.873432, mean score=44.973539, stdev=0.260312  epochs=6, mean epochs=6  time=0:00:01.54\n",
      "#36: score=44.972382, mean score=44.973507, stdev=0.256671  epochs=6, mean epochs=6  time=0:00:01.55\n",
      "#37: score=45.265881, mean score=44.981409, stdev=0.257580  epochs=6, mean epochs=6  time=0:00:01.55\n",
      "#38: score=45.336849, mean score=44.990763, stdev=0.260459  epochs=6, mean epochs=6  time=0:00:01.55\n",
      "#39: score=44.997665, mean score=44.990940, stdev=0.257100  epochs=6, mean epochs=6  time=0:00:01.54\n",
      "#40: score=45.002724, mean score=44.991234, stdev=0.253873  epochs=6, mean epochs=6  time=0:00:01.55\n",
      "#41: score=44.457058, mean score=44.978206, stdev=0.263949  epochs=6, mean epochs=6  time=0:00:01.54\n",
      "#42: score=44.501518, mean score=44.966856, stdev=0.270725  epochs=6, mean epochs=6  time=0:00:01.54\n",
      "#43: score=45.136791, mean score=44.970808, stdev=0.268781  epochs=6, mean epochs=6  time=0:00:01.55\n",
      "#44: score=45.139175, mean score=44.974634, stdev=0.266892  epochs=6, mean epochs=6  time=0:00:01.55\n",
      "#45: score=45.063290, mean score=44.976605, stdev=0.264233  epochs=6, mean epochs=6  time=0:00:01.57\n",
      "#46: score=44.894932, mean score=44.974829, stdev=0.261616  epochs=6, mean epochs=6  time=0:00:01.61\n",
      "#47: score=44.852200, mean score=44.972220, stdev=0.259422  epochs=6, mean epochs=6  time=0:00:01.55\n",
      "#48: score=45.081535, mean score=44.974497, stdev=0.257180  epochs=6, mean epochs=6  time=0:00:01.55\n",
      "#49: score=45.131474, mean score=44.977701, stdev=0.255508  epochs=6, mean epochs=6  time=0:00:01.54\n",
      "#50: score=45.020718, mean score=44.978561, stdev=0.253012  epochs=6, mean epochs=6  time=0:00:01.55\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import statistics\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "SPLITS = 50\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Bootstrap\n",
    "boot = ShuffleSplit(n_splits=SPLITS, test_size=0.1, random_state=42)\n",
    "\n",
    "# Track progress\n",
    "mean_benchmark = []\n",
    "epochs_needed = []\n",
    "num = 0\n",
    "\n",
    "# Loop through samples\n",
    "for train, test in boot.split(x):\n",
    "    start_time = time.time()\n",
    "    num+=1\n",
    "\n",
    "    # Split train and test\n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "\n",
    "    # Define the PyTorch Neural Network\n",
    "    class Net(nn.Module):\n",
    "      def __init__(self, in_count, out_count):\n",
    "          super(Net, self).__init__()\n",
    "          self.fc1 = nn.Linear(in_count, 50)\n",
    "          self.fc2 = nn.Linear(50, 25)\n",
    "          self.fc3 = nn.Linear(25, out_count)\n",
    "          self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "      def forward(self, x):\n",
    "          x = F.relu(self.fc1(x))\n",
    "          x = F.relu(self.fc2(x))\n",
    "          return self.softmax(self.fc3(x))\n",
    "\n",
    "    # Numpy to PyTorch\n",
    "    x_train = torch.Tensor(x_train).float()\n",
    "    y_train = torch.Tensor(y_train).float()\n",
    "\n",
    "    x_test = torch.Tensor(x_test).float().to(device)\n",
    "    y_test = torch.Tensor(y_test).float().to(device)\n",
    "\n",
    "    # Create datasets\n",
    "    dataset_train = TensorDataset(x_train, y_train)\n",
    "    dataloader_train = DataLoader(dataset_train,\\\n",
    "      batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    dataset_test = TensorDataset(x_test, y_test)\n",
    "    dataloader_test = DataLoader(dataset_test,\\\n",
    "      batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Train the network\n",
    "    model = Net(x.shape[1],1).to(device)\n",
    "\n",
    "    # Define the loss function for regression\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    es = EarlyStopping()\n",
    "\n",
    "    epoch = 0\n",
    "    done = False\n",
    "    while epoch<1000 and not done:\n",
    "      epoch += 1\n",
    "      steps = list(enumerate(dataloader_train))\n",
    "      model.train()\n",
    "      for i, (x_batch, y_batch) in steps:\n",
    "        y_batch_pred = model(x_batch.to(device)).flatten()\n",
    "        loss = loss_fn(y_batch_pred, y_batch.to(device))\n",
    "        loss += add_l2_norm_loss(model)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss, current = loss.item(), (i + 1)* len(x_batch)\n",
    "        if i == len(steps)-1:\n",
    "          model.eval()\n",
    "          pred = model(x_test).flatten()\n",
    "          vloss = loss_fn(pred, y_test)\n",
    "          if es(model,vloss): done = True\n",
    "    \n",
    "    model.eval()\n",
    "    pred = model(x_test)\n",
    "  \n",
    "    # Measure this bootstrap's log loss\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred.cpu().detach(),y_test.cpu().detach()))\n",
    "    mean_benchmark.append(float(score))\n",
    "    epochs_needed.append(int(epoch))\n",
    "    m1 = statistics.mean(mean_benchmark)\n",
    "    m2 = statistics.mean(epochs_needed)\n",
    "    mdev = statistics.pstdev(mean_benchmark)\n",
    "    \n",
    "    # Record this iteration\n",
    "    time_took = time.time() - start_time\n",
    "    print(f\"#{num}: score={score:.6f}, mean score={m1:.6f},\"\n",
    "          f\" stdev={mdev:.6f}\", \n",
    "          f\" epochs={epoch}, mean epochs={int(m2)}\", \n",
    "          f\" time={hms_string(time_took)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIFZhOuykn4U"
   },
   "source": [
    "The bootstrapping process for classification is similar, and I present it in the next section.\n",
    "\n",
    "## Bootstrapping for Classification\n",
    "\n",
    "Regression bootstrapping uses the **StratifiedShuffleSplit** class to perform the splits.  This class is similar to **StratifiedKFold** for cross-validation, as the classes are balanced so that the sampling does not affect proportions.  To demonstrate this technique, we will attempt to predict the product column for the **jh-simple-dataset**; the following code loads this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jK8HwVfDkn4U"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Read the data set\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Generate dummies for job\n",
    "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1)\n",
    "df.drop('job', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for area\n",
    "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n",
    "df.drop('area', axis=1, inplace=True)\n",
    "\n",
    "# Missing values for income\n",
    "med = df['income'].median()\n",
    "df['income'] = df['income'].fillna(med)\n",
    "\n",
    "# Standardize ranges\n",
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['age'] = zscore(df['age'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "le = preprocessing.LabelEncoder()\n",
    "x_columns = df.columns.drop('product').drop('id')\n",
    "x = df[x_columns].values\n",
    "y = le.fit_transform(df['product'])\n",
    "products = le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q5JC3-U3JDrA"
   },
   "outputs": [],
   "source": [
    "# Define the PyTorch Neural Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_count, out_count):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_count, 50)\n",
    "        self.fc2 = nn.Linear(50, 25)\n",
    "        self.fc3 = nn.Linear(25, out_count)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.softmax(self.fc3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQSViKNJkn4U"
   },
   "source": [
    "We now run this data through a number of splits specified by the SPLITS variable. We track the average error through each of these splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzu6bqDukn4U",
    "outputId": "2e62d6a7-8607-4bff-c39e-dd5442e70dd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1: score=1.685417, mean score=1.685417,stdev=0.000000, epochs=6, mean epochs=6, time=0:00:01.11\n",
      "#2: score=1.501370, mean score=1.593394,stdev=0.092024, epochs=13, mean epochs=9, time=0:00:02.38\n",
      "#3: score=1.513329, mean score=1.566705,stdev=0.084084, epochs=21, mean epochs=13, time=0:00:03.78\n",
      "#4: score=1.506279, mean score=1.551599,stdev=0.077377, epochs=10, mean epochs=12, time=0:00:01.82\n",
      "#5: score=1.641635, mean score=1.569606,stdev=0.078018, epochs=16, mean epochs=13, time=0:00:02.89\n",
      "#6: score=1.502168, mean score=1.558367,stdev=0.075525, epochs=19, mean epochs=14, time=0:00:03.45\n",
      "#7: score=1.536047, mean score=1.555178,stdev=0.070357, epochs=10, mean epochs=13, time=0:00:01.82\n",
      "#8: score=1.449447, mean score=1.541962,stdev=0.074526, epochs=16, mean epochs=13, time=0:00:02.92\n",
      "#9: score=1.684237, mean score=1.557770,stdev=0.083284, epochs=11, mean epochs=13, time=0:00:02.00\n",
      "#10: score=1.481287, mean score=1.550122,stdev=0.082274, epochs=14, mean epochs=13, time=0:00:02.53\n",
      "#11: score=1.614564, mean score=1.555980,stdev=0.080603, epochs=14, mean epochs=13, time=0:00:02.53\n",
      "#12: score=1.629659, mean score=1.562120,stdev=0.079813, epochs=16, mean epochs=13, time=0:00:02.90\n",
      "#13: score=1.529767, mean score=1.559631,stdev=0.077165, epochs=9, mean epochs=13, time=0:00:01.62\n",
      "#14: score=1.497736, mean score=1.555210,stdev=0.076048, epochs=9, mean epochs=13, time=0:00:01.63\n",
      "#15: score=1.527307, mean score=1.553350,stdev=0.073798, epochs=15, mean epochs=13, time=0:00:02.71\n",
      "#16: score=1.542517, mean score=1.552673,stdev=0.071503, epochs=22, mean epochs=13, time=0:00:03.99\n",
      "#17: score=1.518161, mean score=1.550643,stdev=0.069841, epochs=7, mean epochs=13, time=0:00:01.27\n",
      "#18: score=1.685422, mean score=1.558131,stdev=0.074565, epochs=6, mean epochs=13, time=0:00:01.09\n",
      "#19: score=1.465523, mean score=1.553257,stdev=0.075465, epochs=19, mean epochs=13, time=0:00:03.44\n",
      "#20: score=1.522690, mean score=1.551728,stdev=0.073855, epochs=15, mean epochs=13, time=0:00:02.74\n",
      "#21: score=1.523461, mean score=1.550382,stdev=0.072326, epochs=6, mean epochs=13, time=0:00:01.10\n",
      "#22: score=1.634807, mean score=1.554220,stdev=0.072819, epochs=14, mean epochs=13, time=0:00:02.62\n",
      "#23: score=1.497344, mean score=1.551747,stdev=0.072156, epochs=7, mean epochs=12, time=0:00:01.28\n",
      "#24: score=1.648098, mean score=1.555761,stdev=0.073214, epochs=11, mean epochs=12, time=0:00:02.00\n",
      "#25: score=1.526565, mean score=1.554594,stdev=0.071963, epochs=11, mean epochs=12, time=0:00:02.00\n",
      "#26: score=1.464939, mean score=1.551145,stdev=0.072641, epochs=18, mean epochs=12, time=0:00:03.26\n",
      "#27: score=1.549433, mean score=1.551082,stdev=0.071284, epochs=10, mean epochs=12, time=0:00:01.83\n",
      "#28: score=1.553249, mean score=1.551159,stdev=0.070000, epochs=12, mean epochs=12, time=0:00:02.18\n",
      "#29: score=1.500009, mean score=1.549396,stdev=0.069413, epochs=16, mean epochs=12, time=0:00:02.90\n",
      "#30: score=1.648279, mean score=1.552692,stdev=0.070517, epochs=19, mean epochs=13, time=0:00:03.48\n",
      "#31: score=1.499006, mean score=1.550960,stdev=0.070016, epochs=11, mean epochs=13, time=0:00:02.01\n",
      "#32: score=1.478826, mean score=1.548706,stdev=0.070047, epochs=11, mean epochs=12, time=0:00:02.00\n",
      "#33: score=1.474112, mean score=1.546445,stdev=0.070153, epochs=12, mean epochs=12, time=0:00:02.19\n",
      "#34: score=1.685421, mean score=1.550533,stdev=0.072993, epochs=6, mean epochs=12, time=0:00:01.09\n",
      "#35: score=1.460253, mean score=1.547953,stdev=0.073498, epochs=14, mean epochs=12, time=0:00:02.55\n",
      "#36: score=1.462250, mean score=1.545573,stdev=0.073826, epochs=15, mean epochs=12, time=0:00:02.75\n",
      "#37: score=1.552483, mean score=1.545759,stdev=0.072830, epochs=7, mean epochs=12, time=0:00:01.28\n",
      "#38: score=1.515605, mean score=1.544966,stdev=0.072027, epochs=11, mean epochs=12, time=0:00:01.98\n",
      "#39: score=1.527568, mean score=1.544520,stdev=0.071151, epochs=15, mean epochs=12, time=0:00:02.73\n",
      "#40: score=1.685420, mean score=1.548042,stdev=0.073620, epochs=7, mean epochs=12, time=0:00:01.28\n",
      "#41: score=1.481670, mean score=1.546423,stdev=0.073433, epochs=25, mean epochs=12, time=0:00:04.56\n",
      "#42: score=1.469832, mean score=1.544600,stdev=0.073488, epochs=25, mean epochs=13, time=0:00:04.54\n",
      "#43: score=1.649203, mean score=1.547033,stdev=0.074319, epochs=13, mean epochs=13, time=0:00:02.37\n",
      "#44: score=1.563721, mean score=1.547412,stdev=0.073512, epochs=21, mean epochs=13, time=0:00:03.81\n",
      "#45: score=1.468804, mean score=1.545665,stdev=0.073608, epochs=12, mean epochs=13, time=0:00:02.18\n",
      "#46: score=1.484584, mean score=1.544337,stdev=0.073347, epochs=9, mean epochs=13, time=0:00:01.65\n",
      "#47: score=1.685406, mean score=1.547339,stdev=0.075364, epochs=6, mean epochs=13, time=0:00:01.10\n",
      "#48: score=1.507237, mean score=1.546503,stdev=0.074794, epochs=19, mean epochs=13, time=0:00:03.45\n",
      "#49: score=1.546654, mean score=1.546506,stdev=0.074027, epochs=20, mean epochs=13, time=0:00:03.82\n",
      "#50: score=1.502753, mean score=1.545631,stdev=0.073539, epochs=13, mean epochs=13, time=0:00:02.49\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import tqdm\n",
    "\n",
    "SPLITS = 50\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Bootstrap\n",
    "boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.1, \n",
    "                                random_state=42)\n",
    "\n",
    "# Track progress\n",
    "mean_benchmark = []\n",
    "epochs_needed = []\n",
    "num = 0\n",
    "\n",
    "# Loop through samples\n",
    "for train, test in boot.split(x,df['product']):\n",
    "    start_time = time.time()\n",
    "    num+=1\n",
    "\n",
    "    # Split train and test\n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "\n",
    "    # Numpy to PyTorch\n",
    "    x_train = torch.Tensor(x_train).float()\n",
    "    y_train = torch.Tensor(y_train).long()\n",
    "\n",
    "    x_test = torch.Tensor(x_test).float().to(device)\n",
    "    y_test = torch.Tensor(y_test).long().to(device)\n",
    "\n",
    "    # Create datasets\n",
    "    dataset_train = TensorDataset(x_train, y_train)\n",
    "    dataloader_train = DataLoader(dataset_train,\\\n",
    "      batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    dataset_test = TensorDataset(x_test, y_test)\n",
    "    dataloader_test = DataLoader(dataset_test,\\\n",
    "      batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Create model\n",
    "    model = Net(x.shape[1],len(products)).to(device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()# cross entropy loss\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    es = EarlyStopping()\n",
    "\n",
    "    epoch = 0\n",
    "    done = False\n",
    "    while epoch<1000 and not done:\n",
    "      epoch += 1\n",
    "      steps = list(enumerate(dataloader_train))\n",
    "      model.train()\n",
    "      for i, (x_batch, y_batch) in steps:\n",
    "        y_batch_pred = model(x_batch.to(device))\n",
    "        loss = loss_fn(y_batch_pred, y_batch.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss, current = loss.item(), (i + 1)* len(x_batch)\n",
    "        if i == len(steps)-1:\n",
    "          model.eval()\n",
    "          pred = model(x_test)\n",
    "          vloss = loss_fn(pred, y_test)\n",
    "          if es(model,vloss): done = True\n",
    "    \n",
    "    model.eval()\n",
    "    pred = model(x_test)\n",
    "    score = loss_fn(pred, y_test)\n",
    "    mean_benchmark.append(float(score))\n",
    "    epochs_needed.append(int(epoch))\n",
    "    m1 = statistics.mean(mean_benchmark)\n",
    "    m2 = statistics.mean(epochs_needed)\n",
    "    mdev = statistics.pstdev(mean_benchmark)\n",
    "    \n",
    "    # Record this iteration\n",
    "    time_took = time.time() - start_time\n",
    "    print(f\"#{num}: score={score:.6f}, mean score={m1:.6f},\" +\\\n",
    "          f\"stdev={mdev:.6f}, epochs={epoch}, mean epochs={int(m2)},\" +\\\n",
    "          f\" time={hms_string(time_took)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CfKgQYYqrUmi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHxvxKl0kn4V"
   },
   "source": [
    "## Benchmarking\n",
    "\n",
    "Now that we've seen how to bootstrap with both classification and regression, we can start to try to optimize the hyperparameters for the **jh-simple-dataset** data.  For this example, we will encode for classification of the product column.  Evaluation will be in log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VdKviOzhkn4V"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Read the data set\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Generate dummies for job\n",
    "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1)\n",
    "df.drop('job', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for area\n",
    "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],\n",
    "               axis=1)\n",
    "df.drop('area', axis=1, inplace=True)\n",
    "\n",
    "# Missing values for income\n",
    "med = df['income'].median()\n",
    "df['income'] = df['income'].fillna(med)\n",
    "\n",
    "# Standardize ranges\n",
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['age'] = zscore(df['age'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x_columns = df.columns.drop('product').drop('id')\n",
    "x = df[x_columns].values\n",
    "dummies = pd.get_dummies(df['product']) # Classification\n",
    "products = dummies.columns\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKIKR9vFkn4V"
   },
   "source": [
    "I performed some optimization, and the code has the best settings that I could determine. Later in this book, we will see how we can use an automatic process to optimize the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PdIXT00qIp9h"
   },
   "outputs": [],
   "source": [
    "# Define the PyTorch Neural Network\n",
    "class Net(nn.Module):\n",
    "  def __init__(self, in_count, out_count):\n",
    "      super(Net, self).__init__()\n",
    "      self.fc1 = nn.Linear(in_count, 50)\n",
    "      self.fc2 = nn.Linear(50, 25)\n",
    "      self.fc3 = nn.Linear(25, out_count)\n",
    "      self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "      x = F.relu(self.fc1(x))\n",
    "      x = F.relu(self.fc2(x))\n",
    "      self.dropout = nn.Dropout(0.25)\n",
    "      return self.softmax(self.fc3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nk4w1p42kn4V",
    "outputId": "d4569929-29e7-4c67-d1a9-ec36161a0b79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1: score=1.532517, mean score=1.532517,stdev=0.000000, epochs=7, mean epochs=7, time=0:00:01.38\n",
      "#2: score=1.471129, mean score=1.501823,stdev=0.030694, epochs=9, mean epochs=8, time=0:00:01.77\n",
      "#3: score=1.519374, mean score=1.507673,stdev=0.026392, epochs=11, mean epochs=9, time=0:00:02.16\n",
      "#4: score=1.632895, mean score=1.538979,stdev=0.058843, epochs=17, mean epochs=11, time=0:00:03.31\n",
      "#5: score=1.475250, mean score=1.526233,stdev=0.058479, epochs=12, mean epochs=11, time=0:00:02.33\n",
      "#6: score=1.501268, mean score=1.522072,stdev=0.054188, epochs=26, mean epochs=13, time=0:00:05.07\n",
      "#7: score=1.542092, mean score=1.524932,stdev=0.050655, epochs=10, mean epochs=13, time=0:00:02.03\n",
      "#8: score=1.499199, mean score=1.521715,stdev=0.048142, epochs=7, mean epochs=12, time=0:00:01.38\n",
      "#9: score=1.564127, mean score=1.526428,stdev=0.047305, epochs=6, mean epochs=11, time=0:00:01.17\n",
      "#10: score=1.488320, mean score=1.522617,stdev=0.046311, epochs=10, mean epochs=11, time=0:00:01.95\n",
      "#11: score=1.661053, mean score=1.535202,stdev=0.059444, epochs=7, mean epochs=11, time=0:00:01.37\n",
      "#12: score=1.464860, mean score=1.529340,stdev=0.060142, epochs=21, mean epochs=11, time=0:00:04.11\n",
      "#13: score=1.645425, mean score=1.538270,stdev=0.065542, epochs=12, mean epochs=11, time=0:00:02.34\n",
      "#14: score=1.495330, mean score=1.535203,stdev=0.064118, epochs=13, mean epochs=12, time=0:00:02.54\n",
      "#15: score=1.509481, mean score=1.533488,stdev=0.062276, epochs=17, mean epochs=12, time=0:00:03.32\n",
      "#16: score=1.556598, mean score=1.534932,stdev=0.060557, epochs=8, mean epochs=12, time=0:00:01.58\n",
      "#17: score=1.498228, mean score=1.532773,stdev=0.059380, epochs=12, mean epochs=12, time=0:00:02.37\n",
      "#18: score=1.489668, mean score=1.530379,stdev=0.058546, epochs=30, mean epochs=13, time=0:00:05.86\n",
      "#19: score=1.470261, mean score=1.527215,stdev=0.058544, epochs=8, mean epochs=12, time=0:00:01.57\n",
      "#20: score=1.638136, mean score=1.532761,stdev=0.061972, epochs=9, mean epochs=12, time=0:00:01.77\n",
      "#21: score=1.505438, mean score=1.531460,stdev=0.060757, epochs=14, mean epochs=12, time=0:00:02.75\n",
      "#22: score=1.641303, mean score=1.536452,stdev=0.063617, epochs=15, mean epochs=12, time=0:00:02.94\n",
      "#23: score=1.685422, mean score=1.542929,stdev=0.069240, epochs=6, mean epochs=12, time=0:00:01.19\n",
      "#24: score=1.488076, mean score=1.540644,stdev=0.068662, epochs=12, mean epochs=12, time=0:00:02.35\n",
      "#25: score=1.625385, mean score=1.544034,stdev=0.069294, epochs=8, mean epochs=12, time=0:00:01.66\n",
      "#26: score=1.509795, mean score=1.542717,stdev=0.068267, epochs=21, mean epochs=12, time=0:00:04.14\n",
      "#27: score=1.503974, mean score=1.541282,stdev=0.067389, epochs=11, mean epochs=12, time=0:00:02.14\n",
      "#28: score=1.559128, mean score=1.541919,stdev=0.066258, epochs=8, mean epochs=12, time=0:00:01.57\n",
      "#29: score=1.508584, mean score=1.540770,stdev=0.065389, epochs=10, mean epochs=12, time=0:00:01.96\n",
      "#30: score=1.479172, mean score=1.538716,stdev=0.065234, epochs=15, mean epochs=12, time=0:00:02.96\n",
      "#31: score=1.505009, mean score=1.537629,stdev=0.064449, epochs=7, mean epochs=12, time=0:00:01.37\n",
      "#32: score=1.685422, mean score=1.542248,stdev=0.068448, epochs=6, mean epochs=12, time=0:00:01.18\n",
      "#33: score=1.431405, mean score=1.538889,stdev=0.070030, epochs=17, mean epochs=12, time=0:00:03.35\n",
      "#34: score=1.502211, mean score=1.537810,stdev=0.069270, epochs=10, mean epochs=12, time=0:00:01.97\n",
      "#35: score=1.455058, mean score=1.535446,stdev=0.069651, epochs=10, mean epochs=12, time=0:00:01.96\n",
      "#36: score=1.486117, mean score=1.534075,stdev=0.069154, epochs=10, mean epochs=12, time=0:00:01.97\n",
      "#37: score=1.516184, mean score=1.533592,stdev=0.068274, epochs=21, mean epochs=12, time=0:00:04.14\n",
      "#38: score=1.503402, mean score=1.532797,stdev=0.067543, epochs=9, mean epochs=12, time=0:00:01.77\n",
      "#39: score=1.516385, mean score=1.532377,stdev=0.066722, epochs=9, mean epochs=12, time=0:00:01.77\n",
      "#40: score=1.516191, mean score=1.531972,stdev=0.065931, epochs=14, mean epochs=12, time=0:00:02.76\n",
      "#41: score=1.447868, mean score=1.529921,stdev=0.066402, epochs=26, mean epochs=12, time=0:00:05.10\n",
      "#42: score=1.478915, mean score=1.528706,stdev=0.066066, epochs=26, mean epochs=12, time=0:00:05.08\n",
      "#43: score=1.471449, mean score=1.527375,stdev=0.065861, epochs=9, mean epochs=12, time=0:00:01.77\n",
      "#44: score=1.554296, mean score=1.527986,stdev=0.065232, epochs=8, mean epochs=12, time=0:00:01.57\n",
      "#45: score=1.508495, mean score=1.527553,stdev=0.064567, epochs=11, mean epochs=12, time=0:00:02.18\n",
      "#46: score=1.630353, mean score=1.529788,stdev=0.065597, epochs=13, mean epochs=12, time=0:00:02.57\n",
      "#47: score=1.464513, mean score=1.528399,stdev=0.065576, epochs=11, mean epochs=12, time=0:00:02.18\n",
      "#48: score=1.500501, mean score=1.527818,stdev=0.065011, epochs=21, mean epochs=12, time=0:00:04.12\n",
      "#49: score=1.636222, mean score=1.530030,stdev=0.066145, epochs=9, mean epochs=12, time=0:00:01.77\n",
      "#50: score=1.503718, mean score=1.529504,stdev=0.065583, epochs=12, mean epochs=12, time=0:00:02.36\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "SPLITS = 50\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Bootstrap\n",
    "boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.1, \n",
    "                                random_state=42)\n",
    "\n",
    "# Track progress\n",
    "mean_benchmark = []\n",
    "epochs_needed = []\n",
    "num = 0\n",
    "\n",
    "# Loop through samples\n",
    "for train, test in boot.split(x,df['product']):\n",
    "    start_time = time.time()\n",
    "    num+=1\n",
    "\n",
    "    # Split train and test\n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "\n",
    "    # Numpy to PyTorch\n",
    "    x_train = torch.Tensor(x_train).float()\n",
    "    y_train = torch.Tensor(y_train).float()\n",
    "\n",
    "    x_test = torch.Tensor(x_test).float().to(device)\n",
    "    y_test = torch.Tensor(y_test).float().to(device)\n",
    "\n",
    "    # Create datasets\n",
    "    dataset_train = TensorDataset(x_train, y_train)\n",
    "    dataloader_train = DataLoader(dataset_train,\\\n",
    "      batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    dataset_test = TensorDataset(x_test, y_test)\n",
    "    dataloader_test = DataLoader(dataset_test,\\\n",
    "      batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Train the network\n",
    "    model = Net(x.shape[1],len(products)).to(device)\n",
    "\n",
    "    # Define the loss function for classification\n",
    "    loss_fn = nn.CrossEntropyLoss()# cross entropy loss\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    es = EarlyStopping()\n",
    "\n",
    "    epoch = 0\n",
    "    done = False\n",
    "    while epoch<1000 and not done:\n",
    "      epoch += 1\n",
    "      steps = list(enumerate(dataloader_train))\n",
    "      model.train()\n",
    "      for i, (x_batch, y_batch) in steps:\n",
    "        y_batch_pred = model(x_batch.to(device))\n",
    "        loss = loss_fn(y_batch_pred, y_batch.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss, current = loss.item(), (i + 1)* len(x_batch)\n",
    "        if i == len(steps)-1:\n",
    "          model.eval()\n",
    "          pred = model(x_test)\n",
    "          vloss = loss_fn(pred, y_test)\n",
    "          if es(model,vloss): done = True\n",
    "    \n",
    "    model.eval()\n",
    "    pred = model(x_test)\n",
    "    score = loss_fn(pred, y_test)\n",
    "    mean_benchmark.append(float(score))\n",
    "    epochs_needed.append(int(epoch))\n",
    "    m1 = statistics.mean(mean_benchmark)\n",
    "    m2 = statistics.mean(epochs_needed)\n",
    "    mdev = statistics.pstdev(mean_benchmark)\n",
    "    \n",
    "    # Record this iteration\n",
    "    time_took = time.time() - start_time\n",
    "    print(f\"#{num}: score={score:.6f}, mean score={m1:.6f},\" +\\\n",
    "          f\"stdev={mdev:.6f}, epochs={epoch}, mean epochs={int(m2)},\" +\\\n",
    "          f\" time={hms_string(time_took)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "new t81_558_class_05_5_bootstrap.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
