{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIsM98AYzLUw"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_05_4_dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8PaVkSJzLUy"
      },
      "source": [
        "# T81-558: Applications of Deep Neural Networks\n",
        "**Module 5: Regularization and Dropout**\n",
        "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
        "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhdSqdqPzLUz"
      },
      "source": [
        "# Module 5 Material\n",
        "\n",
        "* Part 5.1: Part 5.1: Introduction to Regularization: Ridge and Lasso [[Video]](https://www.youtube.com/watch?v=jfgRtCYjoBs&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_05_1_reg_ridge_lasso.ipynb)\n",
        "* Part 5.2: Using K-Fold Cross Validation with Keras [[Video]](https://www.youtube.com/watch?v=maiQf8ray_s&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_05_2_kfold.ipynb)\n",
        "* Part 5.3: Using L1 and L2 Regularization with Keras to Decrease Overfitting [[Video]](https://www.youtube.com/watch?v=JEWzWv1fBFQ&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_05_3_keras_l1_l2.ipynb)\n",
        "* **Part 5.4: Drop Out for Keras to Decrease Overfitting** [[Video]](https://www.youtube.com/watch?v=bRyOi0L6Rs8&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_05_4_dropout.ipynb)\n",
        "* Part 5.5: Benchmarking Keras Deep Learning Regularization Techniques [[Video]](https://www.youtube.com/watch?v=1NLBwPumUAs&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_05_5_bootstrap.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3C6jZOXzLUz"
      },
      "source": [
        "# Google CoLab Instructions\n",
        "\n",
        "The following code ensures that Google CoLab is running and maps Google Drive if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i-8qE8ZTzLUz",
        "outputId": "5e83e57d-ec46-463b-a155-329b14335d79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: using Google CoLab\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "try:\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False\n",
        "\n",
        "import io\n",
        "import copy\n",
        "\n",
        "# Define class for early stopping. For more information, see module 3.4.\n",
        "class EarlyStopping():\n",
        "  def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.restore_best_weights = restore_best_weights\n",
        "    self.best_model = None\n",
        "    self.best_loss = None\n",
        "    self.counter = 0\n",
        "    self.status = \"\"\n",
        "    \n",
        "  def __call__(self, model, val_loss):\n",
        "    if self.best_loss == None:\n",
        "      self.best_loss = val_loss\n",
        "      self.best_model = copy.deepcopy(model)\n",
        "    elif self.best_loss - val_loss > self.min_delta:\n",
        "      self.best_loss = val_loss\n",
        "      self.counter = 0\n",
        "      self.best_model.load_state_dict(model.state_dict())\n",
        "    elif self.best_loss - val_loss < self.min_delta:\n",
        "      self.counter += 1\n",
        "      if self.counter >= self.patience:\n",
        "        self.status = f\"Stopped on {self.counter}\"\n",
        "        if self.restore_best_weights:\n",
        "          model.load_state_dict(self.best_model.state_dict())\n",
        "        return True\n",
        "    self.status = f\"{self.counter}/{self.patience}\"\n",
        "    return False\n",
        "\n",
        "# Make use of a GPU if one is available.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHsYiPFszLU0"
      },
      "source": [
        "# Part 5.4: Drop Out for PyTorch to Decrease Overfitting\n",
        "\n",
        "Hinton, Srivastava, Krizhevsky, Sutskever, & Salakhutdinov (2012) introduced the dropout regularization algorithm. [[Cite:srivastava2014dropout]](http://www.jmlr.org/papers/volume15/nandan14a/nandan14a.pdf) Although dropout works differently than L1 and L2, it accomplishes the same goal—the prevention of overfitting. However, the algorithm does the task by actually removing neurons and connections—at least temporarily. Unlike L1 and L2, no weight penalty is added. Dropout does not directly seek to train small weights.\n",
        "Dropout works by causing hidden neurons of the neural network to be unavailable during part of the training. Dropping part of the neural network causes the remaining portion to be trained to still achieve a good score even without the dropped neurons. This technique decreases co-adaptation between neurons, which results in less overfitting. \n",
        "\n",
        "Most neural network frameworks implement dropout as a separate layer. Dropout layers function like a regular, densely connected neural network layer. The only difference is that the dropout layers will periodically drop some of their neurons during training. You can use dropout layers on regular feedforward neural networks. \n",
        "\n",
        "The program implements a dropout layer as a dense layer that can eliminate some of its neurons. Contrary to popular belief about the dropout layer, the program does not permanently remove these discarded neurons. A dropout layer does not lose any of its neurons during the training process, and it will still have the same number of neurons after training. In this way, the program only temporarily masks the neurons rather than dropping them. \n",
        "Figure 5.DROPOUT shows how a dropout layer might be situated with other layers.\n",
        "\n",
        "**Figure 5.DROPOUT: Dropout Regularization**\n",
        "![Dropout Regularization](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_9_dropout.png \"Dropout Regularization\")\n",
        "\n",
        "The discarded neurons and their connections are shown as dashed lines. The input layer has two input neurons as well as a bias neuron. The second layer is a dense layer with three neurons and a bias neuron. The third layer is a dropout layer with six regular neurons even though the program has dropped 50% of them. While the program drops these neurons, it neither calculates nor trains them. However, the final neural network will use all of these neurons for the output. As previously mentioned, the program only temporarily discards the neurons. \n",
        "\n",
        "The program chooses different sets of neurons from the dropout layer during subsequent training iterations. Although we chose a probability of 50% for dropout, the computer will not necessarily drop three neurons. It is as if we flipped a coin for each of the dropout candidate neurons to choose if that neuron was dropped out. You must know that the program should never drop the bias neuron. Only the regular neurons on a dropout layer are candidates.\n",
        "The implementation of the training algorithm influences the process of discarding neurons. The dropout set frequently changes once per training iteration or batch. The program can also provide intervals where all neurons are present. Some neural network frameworks give additional hyper-parameters to allow you to specify exactly the rate of this interval. \n",
        "\n",
        "Why dropout is capable of decreasing overfitting is a common question. The answer is that dropout can reduce the chance of codependency developing between two neurons. Two neurons that develop codependency will not be able to operate effectively when one is dropped out. As a result, the neural network can no longer rely on the presence of every neuron, and it trains accordingly. This characteristic decreases its ability to memorize the information presented, thereby forcing generalization.\n",
        "\n",
        "Dropout also decreases overfitting by forcing a bootstrapping process upon the neural network. Bootstrapping is a prevalent ensemble technique. Ensembling is a technique of machine learning that combines multiple models to produce a better result than those achieved by individual models. The ensemble is a term that originates from the musical ensembles in which the final music product that the audience hears is the combination of many instruments.  \n",
        "\n",
        "Bootstrapping is one of the most simple ensemble techniques. The bootstrapping programmer simply trains several neural networks to perform precisely the same task. However, each neural network will perform differently because of some training techniques and the random numbers used in the neural network weight initialization. The difference in weights causes the performance variance. The output from this ensemble of neural networks becomes the average output of the members taken together. This process decreases overfitting through the consensus of differently trained neural networks.  \n",
        "\n",
        "Dropout works somewhat like bootstrapping. You might think of each neural network that results from a different set of neurons being dropped out as an individual member in an ensemble. As training progresses, the program creates more neural networks in this way. However, dropout does not require the same amount of processing as bootstrapping. The new neural networks created are temporary; they exist only for a training iteration. The final result is also a single neural network rather than an ensemble of neural networks to be averaged together.\n",
        "\n",
        "The following animation shows how dropout works: [animation link](https://yusugomori.com/projects/deep-learning/dropout-relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y9o5zQdEzLU1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Read the data set\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
        "    na_values=['NA','?'])\n",
        "\n",
        "# Generate dummies for job\n",
        "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1)\n",
        "df.drop('job', axis=1, inplace=True)\n",
        "\n",
        "# Generate dummies for area\n",
        "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n",
        "df.drop('area', axis=1, inplace=True)\n",
        "\n",
        "# Missing values for income\n",
        "med = df['income'].median()\n",
        "df['income'] = df['income'].fillna(med)\n",
        "\n",
        "# Standardize ranges\n",
        "df['income'] = zscore(df['income'])\n",
        "df['aspect'] = zscore(df['aspect'])\n",
        "df['save_rate'] = zscore(df['save_rate'])\n",
        "df['age'] = zscore(df['age'])\n",
        "df['subscriptions'] = zscore(df['subscriptions'])\n",
        "\n",
        "# Convert to numpy - Classification\n",
        "x_columns = df.columns.drop('product').drop('id')\n",
        "x = df[x_columns].values\n",
        "dummies = pd.get_dummies(df['product']) # Classification\n",
        "products = dummies.columns\n",
        "y = dummies.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb4NjZ85zLU2"
      },
      "source": [
        "Now we will see how to apply dropout to classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_6OCib4LzLU3",
        "outputId": "89d6b51c-2a57-4753-bdbe-30ec1d0b52ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold #1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1, tloss: 1.402353048324585, vloss: 1.644313, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 96.17it/s]\n",
            "Epoch: 2, tloss: 1.4827700853347778, vloss: 1.514132, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 83.41it/s]\n",
            "Epoch: 3, tloss: 1.3619990348815918, vloss: 1.483695, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 94.75it/s]\n",
            "Epoch: 4, tloss: 1.5625371932983398, vloss: 1.489477, EStop:[1/5]: 100%|██████████| 100/100 [00:01<00:00, 90.33it/s] \n",
            "Epoch: 5, tloss: 1.4772801399230957, vloss: 1.511736, EStop:[2/5]: 100%|██████████| 100/100 [00:01<00:00, 93.77it/s]\n",
            "Epoch: 6, tloss: 1.536592721939087, vloss: 1.523030, EStop:[3/5]: 100%|██████████| 100/100 [00:01<00:00, 71.57it/s]\n",
            "Epoch: 7, tloss: 1.4883153438568115, vloss: 1.490681, EStop:[4/5]: 100%|██████████| 100/100 [00:01<00:00, 79.86it/s]\n",
            "Epoch: 8, tloss: 1.6731081008911133, vloss: 1.516463, EStop:[Stopped on 5]: 100%|██████████| 100/100 [00:00<00:00, 114.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold score (accuracy): 0.68\n",
            "Fold #2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1, tloss: 1.3519165515899658, vloss: 1.704222, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 93.50it/s]\n",
            "Epoch: 2, tloss: 1.5541309118270874, vloss: 1.683767, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 102.28it/s]\n",
            "Epoch: 3, tloss: 1.4977548122406006, vloss: 1.606455, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 91.68it/s]\n",
            "Epoch: 4, tloss: 1.5953760147094727, vloss: 1.591688, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 97.96it/s] \n",
            "Epoch: 5, tloss: 1.5990331172943115, vloss: 1.586289, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 95.39it/s]\n",
            "Epoch: 6, tloss: 1.653628945350647, vloss: 1.660341, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 115.44it/s]\n",
            "Epoch: 7, tloss: 1.6890020370483398, vloss: 1.529499, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 101.93it/s]\n",
            "Epoch: 8, tloss: 1.518317699432373, vloss: 1.525989, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 98.40it/s] \n",
            "Epoch: 9, tloss: 1.46916663646698, vloss: 1.582204, EStop:[1/5]: 100%|██████████| 100/100 [00:01<00:00, 77.12it/s]\n",
            "Epoch: 10, tloss: 1.4112070798873901, vloss: 1.537822, EStop:[2/5]: 100%|██████████| 100/100 [00:00<00:00, 101.86it/s]\n",
            "Epoch: 11, tloss: 1.4777454137802124, vloss: 1.551236, EStop:[3/5]: 100%|██████████| 100/100 [00:01<00:00, 81.08it/s]\n",
            "Epoch: 12, tloss: 1.7275457382202148, vloss: 1.524963, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 83.04it/s]\n",
            "Epoch: 13, tloss: 1.5407646894454956, vloss: 1.522276, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 93.99it/s]\n",
            "Epoch: 14, tloss: 1.4910801649093628, vloss: 1.510159, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 67.43it/s]\n",
            "Epoch: 15, tloss: 1.641401767730713, vloss: 1.521334, EStop:[1/5]: 100%|██████████| 100/100 [00:01<00:00, 86.31it/s]\n",
            "Epoch: 16, tloss: 1.4275187253952026, vloss: 1.535885, EStop:[2/5]: 100%|██████████| 100/100 [00:01<00:00, 82.27it/s]\n",
            "Epoch: 17, tloss: 1.4154226779937744, vloss: 1.522024, EStop:[3/5]: 100%|██████████| 100/100 [00:01<00:00, 88.02it/s]\n",
            "Epoch: 18, tloss: 1.3520821332931519, vloss: 1.518058, EStop:[4/5]: 100%|██████████| 100/100 [00:01<00:00, 88.46it/s]\n",
            "Epoch: 19, tloss: 1.5458141565322876, vloss: 1.545981, EStop:[Stopped on 5]: 100%|██████████| 100/100 [00:01<00:00, 80.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold score (accuracy): 0.6575\n",
            "Fold #3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1, tloss: 1.665410041809082, vloss: 1.680460, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 71.28it/s]\n",
            "Epoch: 2, tloss: 1.5456373691558838, vloss: 1.666368, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 104.22it/s]\n",
            "Epoch: 3, tloss: 1.6036372184753418, vloss: 1.638480, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 140.57it/s]\n",
            "Epoch: 4, tloss: 1.5593489408493042, vloss: 1.636862, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 131.07it/s]\n",
            "Epoch: 5, tloss: 1.7288930416107178, vloss: 1.636376, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 137.92it/s]\n",
            "Epoch: 6, tloss: 1.5479084253311157, vloss: 1.635964, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 145.53it/s]\n",
            "Epoch: 7, tloss: 1.7553894519805908, vloss: 1.642339, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 141.19it/s]\n",
            "Epoch: 8, tloss: 1.7903996706008911, vloss: 1.635041, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 136.74it/s]\n",
            "Epoch: 9, tloss: 1.6186096668243408, vloss: 1.637189, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 141.49it/s]\n",
            "Epoch: 10, tloss: 1.727574348449707, vloss: 1.636207, EStop:[2/5]: 100%|██████████| 100/100 [00:00<00:00, 136.72it/s]\n",
            "Epoch: 11, tloss: 1.602921962738037, vloss: 1.635422, EStop:[3/5]: 100%|██████████| 100/100 [00:00<00:00, 131.42it/s]\n",
            "Epoch: 12, tloss: 1.602921962738037, vloss: 1.635416, EStop:[4/5]: 100%|██████████| 100/100 [00:00<00:00, 137.94it/s]\n",
            "Epoch: 13, tloss: 1.5867745876312256, vloss: 1.645969, EStop:[Stopped on 5]: 100%|██████████| 100/100 [00:00<00:00, 132.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold score (accuracy): 0.53\n",
            "Fold #4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1, tloss: 1.790421485900879, vloss: 1.662916, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 131.63it/s]\n",
            "Epoch: 2, tloss: 1.727921962738037, vloss: 1.662842, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 136.79it/s]\n",
            "Epoch: 3, tloss: 1.8544259071350098, vloss: 1.640962, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 147.57it/s]\n",
            "Epoch: 4, tloss: 1.6669265031814575, vloss: 1.640880, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 143.79it/s]\n",
            "Epoch: 5, tloss: 1.7237110137939453, vloss: 1.630943, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 147.12it/s]\n",
            "Epoch: 6, tloss: 1.727921962738037, vloss: 1.629874, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 149.64it/s]\n",
            "Epoch: 7, tloss: 1.790421962738037, vloss: 1.630765, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 136.72it/s]\n",
            "Epoch: 8, tloss: 1.6653416156768799, vloss: 1.629217, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 142.13it/s]\n",
            "Epoch: 9, tloss: 1.4778081178665161, vloss: 1.627920, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 141.67it/s]\n",
            "Epoch: 10, tloss: 1.7903153896331787, vloss: 1.627880, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 134.34it/s]\n",
            "Epoch: 11, tloss: 1.665421962738037, vloss: 1.627749, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 133.95it/s]\n",
            "Epoch: 12, tloss: 1.727921962738037, vloss: 1.627577, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 143.10it/s]\n",
            "Epoch: 13, tloss: 1.5404306650161743, vloss: 1.625451, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 134.80it/s]\n",
            "Epoch: 14, tloss: 1.727921962738037, vloss: 1.630194, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 133.33it/s]\n",
            "Epoch: 15, tloss: 1.417159914970398, vloss: 1.624572, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 130.76it/s]\n",
            "Epoch: 16, tloss: 1.6653616428375244, vloss: 1.628956, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 135.65it/s]\n",
            "Epoch: 17, tloss: 1.7279223203659058, vloss: 1.628276, EStop:[2/5]: 100%|██████████| 100/100 [00:00<00:00, 139.32it/s]\n",
            "Epoch: 18, tloss: 1.6035456657409668, vloss: 1.625253, EStop:[3/5]: 100%|██████████| 100/100 [00:00<00:00, 137.06it/s]\n",
            "Epoch: 19, tloss: 1.7904220819473267, vloss: 1.620577, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 146.31it/s]\n",
            "Epoch: 20, tloss: 1.3529220819473267, vloss: 1.623041, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 155.69it/s]\n",
            "Epoch: 21, tloss: 1.540421962738037, vloss: 1.624225, EStop:[2/5]: 100%|██████████| 100/100 [00:00<00:00, 148.77it/s]\n",
            "Epoch: 22, tloss: 1.5404220819473267, vloss: 1.627920, EStop:[3/5]: 100%|██████████| 100/100 [00:00<00:00, 140.27it/s]\n",
            "Epoch: 23, tloss: 1.7279222011566162, vloss: 1.625312, EStop:[4/5]: 100%|██████████| 100/100 [00:00<00:00, 140.17it/s]\n",
            "Epoch: 24, tloss: 1.2904220819473267, vloss: 1.625423, EStop:[Stopped on 5]: 100%|██████████| 100/100 [00:00<00:00, 138.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold score (accuracy): 0.545\n",
            "Fold #5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1, tloss: 1.790421962738037, vloss: 1.690422, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 98.15it/s]\n",
            "Epoch: 2, tloss: 1.7279078960418701, vloss: 1.690422, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 96.23it/s]\n",
            "Epoch: 3, tloss: 1.7920873165130615, vloss: 1.688936, EStop:[0/5]: 100%|██████████| 100/100 [00:01<00:00, 86.22it/s]\n",
            "Epoch: 4, tloss: 1.4543278217315674, vloss: 1.566558, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 115.54it/s]\n",
            "Epoch: 5, tloss: 1.3614859580993652, vloss: 1.560618, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 142.28it/s]\n",
            "Epoch: 6, tloss: 1.5441558361053467, vloss: 1.571875, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 142.49it/s]\n",
            "Epoch: 7, tloss: 1.540421962738037, vloss: 1.607793, EStop:[2/5]: 100%|██████████| 100/100 [00:00<00:00, 146.30it/s]\n",
            "Epoch: 8, tloss: 1.6661550998687744, vloss: 1.525931, EStop:[0/5]: 100%|██████████| 100/100 [00:00<00:00, 139.94it/s]\n",
            "Epoch: 9, tloss: 1.540419340133667, vloss: 1.541427, EStop:[1/5]: 100%|██████████| 100/100 [00:00<00:00, 145.42it/s]\n",
            "Epoch: 10, tloss: 1.5398310422897339, vloss: 1.533000, EStop:[2/5]: 100%|██████████| 100/100 [00:00<00:00, 142.26it/s]\n",
            "Epoch: 11, tloss: 1.5404212474822998, vloss: 1.536508, EStop:[3/5]: 100%|██████████| 100/100 [00:00<00:00, 127.55it/s]\n",
            "Epoch: 12, tloss: 1.4150526523590088, vloss: 1.537857, EStop:[4/5]: 100%|██████████| 100/100 [00:00<00:00, 135.29it/s]\n",
            "Epoch: 13, tloss: 1.4775348901748657, vloss: 1.545473, EStop:[Stopped on 5]: 100%|██████████| 100/100 [00:00<00:00, 131.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold score (accuracy): 0.6425\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from sklearn import preprocessing\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "import tqdm\n",
        "import time\n",
        "\n",
        "EPOCHS=500\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Define the PyTorch Neural Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, in_count, out_count):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_count, 50)\n",
        "        self.fc2 = nn.Linear(50, 25)\n",
        "        self.fc3 = nn.Linear(25, out_count)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.softmax(self.fc3(x))\n",
        "\n",
        "# Cross-Validate\n",
        "kf = KFold(5, shuffle=True, random_state=42) # Use for KFold classification\n",
        "oos_y_list = []\n",
        "oos_pred_list = []\n",
        "\n",
        "fold = 0\n",
        "for train, test in kf.split(x):\n",
        "    fold+=1\n",
        "    print(f\"Fold #{fold}\")\n",
        "        \n",
        "    x_train = x[train]\n",
        "    y_train = y[train]\n",
        "    x_test = x[test]\n",
        "    y_test = y[test]\n",
        "\n",
        "    # Numpy to PyTorch\n",
        "    x_train = torch.Tensor(x_train).float()\n",
        "    y_train = torch.Tensor(y_train).float()\n",
        "\n",
        "    x_test = torch.Tensor(x_test).float().to(device)\n",
        "    y_test = torch.Tensor(y_test).float().to(device)\n",
        "\n",
        "    # Create datasets\n",
        "    dataset_train = TensorDataset(x_train, y_train)\n",
        "    dataloader_train = DataLoader(dataset_train,\\\n",
        "      batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    dataset_test = TensorDataset(x_test, y_test)\n",
        "    dataloader_test = DataLoader(dataset_test,\\\n",
        "      batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # Train the network\n",
        "    model = Net(x.shape[1],len(products)).to(device)\n",
        "\n",
        "    # Define the loss function for classification\n",
        "    loss_fn = nn.CrossEntropyLoss()# cross entropy loss\n",
        "\n",
        "    # Define the optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    es = EarlyStopping()\n",
        "\n",
        "    epoch = 0\n",
        "    done = False\n",
        "    while epoch<1000 and not done:\n",
        "      epoch += 1\n",
        "      steps = list(enumerate(dataloader_train))\n",
        "      pbar = tqdm.tqdm(steps)\n",
        "      model.train()\n",
        "      for i, (x_batch, y_batch) in pbar:\n",
        "        y_batch_pred = model(x_batch.to(device))\n",
        "        loss = loss_fn(y_batch_pred, y_batch.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss, current = loss.item(), (i + 1)* len(x_batch)\n",
        "        if i == len(steps)-1:\n",
        "          model.eval()\n",
        "          pred = model(x_test)\n",
        "          vloss = loss_fn(pred, y_test)\n",
        "          if es(model,vloss): done = True\n",
        "          pbar.set_description(f\"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
        "        else:\n",
        "          pbar.set_description(f\"Epoch: {epoch}, tloss {loss:}\")\n",
        "    \n",
        "    pred = model(x_test)\n",
        "    \n",
        "    oos_y_list.append(y_test.cpu().detach())\n",
        "    oos_pred_list.append(pred.cpu().detach())    \n",
        "\n",
        "    # Measure this fold's RMSE\n",
        "    #score = np.sqrt(metrics.mean_squared_error(pred.cpu().detach(),y_test.cpu().detach()))\n",
        "    #print(f\"Fold score (RMSE): {score}\")\n",
        "\n",
        "    # Measure this fold's accuracy\n",
        "    y_compare = np.argmax(y_test.cpu().detach(),axis=1) # For accuracy calculation\n",
        "    pred = np.argmax(pred.cpu().detach(),axis=1) # For accuracy calculation\n",
        "    score = metrics.accuracy_score(y_compare, pred)\n",
        "    print(f\"Fold score (accuracy): {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3ER_pI6TzLU3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3.9 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Copy of t81_558_class_05_4_dropout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}