{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0O4IkSwA9Bb1"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_04_4_backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zIZZGXB9Bb3"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 4: Training for Tabular Data**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkFsE-UA9Bb3"
   },
   "source": [
    "# Module 4 Material\n",
    "\n",
    "* Part 4.1: Encoding a Feature Vector for PyTorch Deep Learning [[Video]](https://www.youtube.com/watch?v=Vxz-gfs9nMQ&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_04_1_feature_encode.ipynb)\n",
    "* Part 4.2: PyTorch Multiclass Classification for Deep Neural Networks with ROC and AUC [[Video]](https://www.youtube.com/watch?v=-f3bg9dLMks&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_04_2_multi_class.ipynb)\n",
    "* Part 4.3: PyTorch Regression for Deep Neural Networks with RMSE [[Video]](https://www.youtube.com/watch?v=wNhBUC6X5-E&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_04_3_regression.ipynb)\n",
    "* **Part 4.4: Backpropagation, Nesterov Momentum, and ADAM Neural Network Training** [[Video]](https://www.youtube.com/watch?v=VbDg8aBgpck&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_04_4_backprop.ipynb)\n",
    "* Part 4.5: Neural Network RMSE and Log Loss Error Calculation from Scratch [[Video]](https://www.youtube.com/watch?v=wmQX1t2PHJc&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_04_5_rmse_logloss.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WD5_gmH19Bb3"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running and maps Google Drive if needed. We also initialize the PyTorch device to either GPU (if available) or CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ECqe1F3F9Bb4",
    "outputId": "420c32ac-1826-4515-d97f-25edf13ce759"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "try:\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "    \n",
    "# Make use of a GPU if one is available. (see Module 3.2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ek8dXs359Bb4"
   },
   "source": [
    "# Part 4.4: Training Neural Networks\n",
    "\n",
    "Backpropagation [[Cite:rumelhart1986learning]](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf) is one of the most common methods for training a neural network. Rumelhart, Hinton, & Williams introduced backpropagation, and it remains popular today. Programmers frequently train deep neural networks with backpropagation because it scales really well when run on graphical processing units (GPUs). To understand this algorithm for neural networks, we must examine how to train it as well as how it processes a pattern.\n",
    "\n",
    "Researchers have extended classic backpropagation and modified to give rise to many different training algorithms. This section will discuss the most commonly used training algorithms for neural networks. We begin with classic backpropagation and end the chapter with stochastic gradient descent (SGD).\n",
    "\n",
    "Backpropagation is the primary means of determining a neural network's weights during training. Backpropagation works by calculating a weight change amount ($v_t$) for every weight($\\theta$, theta) in the neural network. This value is subtracted from every weight by the following equation: \n",
    "\n",
    "$$ \\theta_t = \\theta_{t-1} - v_t $$\n",
    "\n",
    "We repeat this process for every iteration($t$). The training algorithm determines how we calculate the weight change. Classic backpropagation calculates a gradient ($\\nabla$, nabla) for every weight in the neural network for the neural network's error function ($J$). We scale the gradient by a learning rate ($\\eta$, eta).\n",
    "\n",
    "$$ v_t = \\eta \\nabla_{\\theta_{t-1}} J(\\theta_{t-1}) $$\n",
    "\n",
    "The learning rate is an important concept for backpropagation training. Setting the learning rate can be complex:\n",
    "\n",
    "* Too low a learning rate will usually converge to a reasonable solution; however, the process will be prolonged.\n",
    "* Too high of a learning rate will either fail outright or converge to a higher error than a better learning rate.\n",
    "\n",
    "Common values for learning rate are: 0.1, 0.01, 0.001, etc.\n",
    "\n",
    "Backpropagation is a gradient descent type, and many texts will use these two terms interchangeably. Gradient descent refers to calculating a gradient on each weight in the neural network for each training element. Because the neural network will not output the expected value for a training element, the gradient of each weight will indicate how to modify each weight to achieve the expected output. If the neural network did output exactly what was expected, the gradient for each weight would be 0, indicating that no change to the weight is necessary.\n",
    "\n",
    "The gradient is the derivative of the error function at the weight's current value. The error function measures the distance of the neural network's output from the expected output. We can use gradient descent, a process in which each weight's gradient value can reach even lower values of the error function. \n",
    "  \n",
    "The gradient is the partial derivative of each weight in the neural network concerning the error function. Each weight has a gradient that is the slope of the error function. Weight is a connection between two neurons. Calculating the gradient of the error function allows the training method to determine whether it should increase or decrease the weight. In turn, this determination will decrease the error of the neural network. The error is the difference between the expected output and actual output of the neural network. Many different training methods called propagation-training algorithms utilize gradients. In all of them, the sign of the gradient tells the neural network the following information:\n",
    "\n",
    "* Zero gradient - The weight does not contribute to the neural network's error.\n",
    "* Negative gradient - The algorithm should increase the weight to lower error.\n",
    "* Positive gradient - The algorithm should decrease the weight to lower error.\n",
    "\n",
    "\n",
    "Because many algorithms depend on gradient calculation, we will begin with an analysis of this process. First of all, let's examine the gradient. Essentially, training is a search for the set of weights that will cause the neural network to have the lowest error for a training set. If we had infinite computation resources, we would try every possible combination of weights to determine the one that provided the lowest error during the training. \n",
    "\n",
    "Because we do not have unlimited computing resources, we have to use some shortcuts to prevent the need to examine every possible weight combination. These training methods utilize clever techniques to avoid performing a brute-force search of all weight values. This type of exhaustive search would be impossible because even small networks have an infinite number of weight combinations.\n",
    "\n",
    "Consider a chart that shows the error of a neural network for each possible weight. Figure 4.DRV is a graph that demonstrates the error for a single weight:\n",
    "\n",
    "**Figure 4.DRV: Derivative**\n",
    "![Derivative](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_2_deriv.png \"Derivative\")\n",
    "\n",
    "Looking at this chart, you can easily see that the optimal weight is where the line has the lowest y-value. The problem is that we see only the error for the current value of the weight; we do not see the entire graph because that process would require an exhaustive search. However, we can determine the slope of the error curve at a particular weight. In the above chart, we see the slope of the error curve at 1.5. The straight line barely touches the error curve at 1.5 gives the slope. In this case, the slope, or gradient, is -0.5622. The negative slope indicates that an increase in the weight will lower the error.\n",
    "The gradient is the instantaneous slope of the error function at the specified weight. The derivative of the error curve at that point gives the gradient. This line tells us the steepness of the error function at the given weight.  \n",
    "Derivatives are one of the most fundamental concepts in calculus. For this book, you need to understand that a derivative provides the slope of a function at a specific point. A training technique and this slope can give you the information to adjust the weight for a lower error. Using our working definition of the gradient, we will show how to calculate it.\n",
    "\n",
    "## Momentum Backpropagation\n",
    "\n",
    "Momentum adds another term to the calculation of $v_t$:\n",
    "\n",
    "$$ v_t = \\eta \\nabla_{\\theta_{t-1}} J(\\theta_{t-1}) + \\lambda v_{t-1} $$\n",
    "\n",
    "Like the learning rate, momentum adds another training parameter that scales the effect of momentum. Momentum backpropagation has two training parameters: learning rate ($\\eta$, eta) and momentum ($\\lambda$, lambda). Momentum adds the scaled value of the previous weight change amount ($v_{t-1}$) to the current weight change amount($v_t$).\n",
    "\n",
    "This technique has the effect of adding additional force behind the direction a weight is moving. Figure 4.MTM shows how this might allow the weight to escape local minima.\n",
    "\n",
    "**Figure 4.MTM: Momentum**\n",
    "![Momentum](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_5_momentum.png \"Momentum\")\n",
    "\n",
    "A typical value for momentum is 0.9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0gYS2pw9Bb6"
   },
   "source": [
    "## Batch and Online Backpropagation\n",
    "\n",
    "How often should the weights of a neural network be updated?  We can calculate gradients for a training set element.  These gradients can also be summed together into batches, and the weights updated once per batch.\n",
    "\n",
    "* **Online Training** - Update the weights based on gradients calculated from a single training set element.\n",
    "* **Batch Training** - Update the weights based on the sum of the gradients over all training set elements.\n",
    "* **Batch Size** - Update the weights based on the sum of some batch size of training set elements.\n",
    "* **Mini-Batch Training** - The same as batch size, but with minimal batch size.  Mini-batches are very popular, often in the 32-64 element range.\n",
    "\n",
    "Because the batch size is smaller than the full training set size, it may take several batches to make it completely through the training set.  \n",
    "\n",
    "* **Step/Iteration** - The number of processed batches.\n",
    "* **Epoch** - The number of times the algorithm processed the complete training set.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "Stochastic gradient descent (SGD) is currently one of the most popular neural network training algorithms.  It works very similarly to Batch/Mini-Batch training, except that the batches are made up of a random set of training elements.\n",
    "\n",
    "This technique leads to a very irregular convergence in error during training, as shown in Figure 4.SGD.\n",
    "\n",
    "**Figure 4.SGD: SGD Error**\n",
    "![SGD Error](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_5_sgd_error.png \"SGD Error\")\n",
    "[Image from Wikipedia](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
    "\n",
    "Because the neural network is trained on a random sample of the complete training set each time, the error does not make a smooth transition downward.  However, the error usually does go down.\n",
    "\n",
    "Advantages to SGD include:\n",
    "\n",
    "* Computationally efficient.  Each training step can be relatively fast, even with a huge training set.\n",
    "* Decreases overfitting by focusing on only a portion of the training set each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXIhs07k9Bb6"
   },
   "source": [
    "## Other Techniques\n",
    "\n",
    "One problem with simple backpropagation training algorithms is that they are susceptible to learning rate and momentum. This technique is difficult because:\n",
    "\n",
    "* Learning rate must be adjusted to a small enough level to train an accurate neural network.\n",
    "* Momentum must be large enough to overcome local minima yet small enough not to destabilize the training.\n",
    "* A single learning rate/momentum is often not good enough for the entire training process. It is often helpful to automatically decrease the learning rate as the training progresses.\n",
    "* All weights share a single learning rate/momentum.\n",
    "\n",
    "Other training techniques:\n",
    "\n",
    "* **Resilient Propagation** - Use only the magnitude of the gradient and allow each neuron to learn at its rate. There is no need for learning rate/momentum; however, it only works in full batch mode.\n",
    "* **Nesterov accelerated gradient** - Helps mitigate the risk of choosing a bad mini-batch.\n",
    "* **Adagrad** - Allows an automatically decaying per-weight learning rate and momentum concept.\n",
    "* **Adadelta** - Extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate.\n",
    "* **Non-Gradient Methods** - Non-gradient methods can *sometimes* be useful, though rarely outperform gradient-based backpropagation methods.  These include: [simulated annealing](https://en.wikipedia.org/wiki/Simulated_annealing), [genetic algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm), [particle swarm optimization](https://en.wikipedia.org/wiki/Particle_swarm_optimization), [Nelder Mead](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method), and [many more](https://en.wikipedia.org/wiki/Category:Optimization_algorithms_and_methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5YZB9k89Bb6"
   },
   "source": [
    "## ADAM Update\n",
    "\n",
    "ADAM is the first training algorithm you should try.  It is very effective.  Kingma and Ba (2014) introduced the Adam update rule that derives its name from the adaptive moment estimates. [[Cite:kingma2014adam]](https://arxiv.org/abs/1412.6980)  Adam estimates the first (mean) and second (variance) moments to determine the weight corrections.  Adam begins with an exponentially decaying average of past gradients (m):\n",
    "\n",
    "$$ m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t $$\n",
    "\n",
    "This average accomplishes a similar goal as classic momentum update; however, its value is calculated automatically based on the current gradient ($g_t$).  The update rule then calculates the second moment ($v_t$):\n",
    "\n",
    "$$ v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 $$\n",
    "\n",
    "The values $m_t$ and $v_t$ are estimates of the gradients' first moment (the mean) and the second moment (the uncentered variance).  However, they will be strongly biased towards zero in the initial training cycles.  The first moment’s bias is corrected as follows.\n",
    "\n",
    "$$ \\hat{m}_t = \\frac{m_t}{1-\\beta^t_1} $$\n",
    "\n",
    "Similarly, the second moment is also corrected:\n",
    "\n",
    "$$ \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t} $$\n",
    "\n",
    "These bias-corrected first and second moment estimates are applied to the ultimate Adam update rule, as follows:\n",
    "\n",
    "$$ \\theta_t = \\theta_{t-1} - \\frac{\\alpha \\cdot \\hat{m}_t}{\\sqrt{\\hat{v}_t}+\\eta} \\hat{m}_t $$\n",
    "\n",
    "Adam is very tolerant to initial learning rate (\\alpha) and other training parameters. Kingma and Ba (2014)  propose default values of 0.9 for $\\beta_1$, 0.999 for $\\beta_2$, and 10-8 for $\\eta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ijy1zn09Bb7"
   },
   "source": [
    "## Methods Compared\n",
    "\n",
    "The following image shows how each of these algorithms train. It is animated, so it is not displayed in the printed book, but can be accessed from here: [https://bit.ly/3kykkbn](https://bit.ly/3kykkbn).\n",
    "\n",
    "![Training Techniques](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/contours_evaluation_optimizers.gif \"Training Techniques\")\n",
    "Image credits: [Alec Radford](https://scholar.google.com/citations?user=dOad5HoAAAAJ&hl=en)\n",
    "\n",
    "## Specifying the Update Rule in Keras\n",
    "\n",
    "TensorFlow allows the update rule to be set to one of:\n",
    "\n",
    "* Adagrad\n",
    "* **Adam**\n",
    "* Ftrl\n",
    "* Momentum\n",
    "* RMSProp\n",
    "* **SGD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0oX3ikF_Sos",
    "outputId": "9254f4ba-d96a-4f15-93b7-4db9af6062df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "okgUEy6J_UdC"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import copy\n",
    "\n",
    "class EarlyStopping():\n",
    "  def __init__(self, patience=5, min_delta=1e-3, restore_best_weights=True):\n",
    "    self.patience = patience\n",
    "    self.min_delta = min_delta\n",
    "    self.restore_best_weights = restore_best_weights\n",
    "    self.best_model = None\n",
    "    self.best_loss = None\n",
    "    self.counter = 0\n",
    "    self.status = \"\"\n",
    "    \n",
    "  def __call__(self, model, val_loss):\n",
    "    if self.best_loss == None:\n",
    "      self.best_loss = val_loss\n",
    "      self.best_model = copy.deepcopy(model)\n",
    "    elif self.best_loss - val_loss > self.min_delta:\n",
    "      self.best_loss = val_loss\n",
    "      self.counter = 0\n",
    "      self.best_model.load_state_dict(model.state_dict())\n",
    "    elif self.best_loss - val_loss < self.min_delta:\n",
    "      self.counter += 1\n",
    "      if self.counter >= self.patience:\n",
    "        self.status = f\"Stopped on {self.counter}\"\n",
    "        if self.restore_best_weights:\n",
    "          model.load_state_dict(self.best_model.state_dict())\n",
    "        return True\n",
    "    self.status = f\"{self.counter}/{self.patience}\"\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jm1uoZNh9Bb7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the data set\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n",
    "# Generate dummies for job\n",
    "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1)\n",
    "df.drop('job', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for area\n",
    "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n",
    "df.drop('area', axis=1, inplace=True)\n",
    "\n",
    "# Generate dummies for product\n",
    "df = pd.concat([df,pd.get_dummies(df['product'],prefix=\"product\")],axis=1)\n",
    "df.drop('product', axis=1, inplace=True)\n",
    "\n",
    "# Missing values for income\n",
    "med = df['income'].median()\n",
    "df['income'] = df['income'].fillna(med)\n",
    "\n",
    "# Standardize ranges\n",
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])\n",
    "\n",
    "# Convert to numpy - Classification\n",
    "x_columns = df.columns.drop('age').drop('id')\n",
    "x = df[x_columns].values\n",
    "y = df['age'].values\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "# Define the PyTorch Neural Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_count, out_count):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_count, 50)\n",
    "        self.fc2 = nn.Linear(50, 25)\n",
    "        self.fc3 = nn.Linear(25, out_count)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.softmax(self.fc3(x))\n",
    "\n",
    "# Split into validation and training sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Numpy to Torch Tensor\n",
    "x_train = torch.Tensor(x_train).float()\n",
    "y_train = torch.Tensor(y_train).float()\n",
    "\n",
    "x_test = torch.Tensor(x_test).float().to(device)\n",
    "y_test = torch.Tensor(y_test).float().to(device)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "dataset_train = TensorDataset(x_train, y_train)\n",
    "dataloader_train = DataLoader(dataset_train,\\\n",
    "  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(x_test, y_test)\n",
    "dataloader_test = DataLoader(dataset_test,\\\n",
    "  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = Net(x.shape[1],1).to(device)\n",
    "\n",
    "# Define the loss function for regression\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hhpILNLE9Bb8"
   },
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n9iPRlR7A4r5",
    "outputId": "feb4bd41-0402-4ede-efd1-abd77b650ca8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch: 1, tloss 2048.3125:  96%|█████████▌| 90/94 [00:01<00:00, 93.26it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([500])) that is different to the input size (torch.Size([500, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch: 1, tloss: 1814.3333740234375, vloss: 2009.348145, EStop:[0/5]: 100%|██████████| 94/94 [00:01<00:00, 81.34it/s]\n",
      "Epoch: 2, tloss: 1889.8333740234375, vloss: 2009.348145, EStop:[1/5]: 100%|██████████| 94/94 [00:00<00:00, 100.76it/s]\n",
      "Epoch: 3, tloss: 2009.5833740234375, vloss: 2009.348145, EStop:[2/5]: 100%|██████████| 94/94 [00:00<00:00, 111.32it/s]\n",
      "Epoch: 4, tloss: 2085.083251953125, vloss: 2009.348145, EStop:[3/5]: 100%|██████████| 94/94 [00:00<00:00, 95.35it/s] \n",
      "Epoch: 5, tloss: 2061.75, vloss: 2009.348145, EStop:[4/5]: 100%|██████████| 94/94 [00:01<00:00, 93.02it/s]\n",
      "Epoch: 6, tloss: 2003.0, vloss: 2009.348145, EStop:[Stopped on 5]: 100%|██████████| 94/94 [00:01<00:00, 81.13it/s]\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping()\n",
    "\n",
    "epoch = 0\n",
    "done = False\n",
    "while epoch<1000 and not done:\n",
    "  epoch += 1\n",
    "  steps = list(enumerate(dataloader_train))\n",
    "  pbar = tqdm.tqdm(steps)\n",
    "  model.train()\n",
    "  for i, (x_batch, y_batch) in pbar:\n",
    "    y_batch_pred = model(x_batch.to(device))\n",
    "    loss = loss_fn(y_batch_pred, y_batch.to(device))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss, current = loss.item(), (i + 1)* len(x_batch)\n",
    "    if i == len(steps)-1:\n",
    "      model.eval()\n",
    "      pred = model(x_test)\n",
    "      vloss = loss_fn(pred, y_test)\n",
    "      if es(model,vloss): done = True\n",
    "      pbar.set_description(f\"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "    else:\n",
    "      pbar.set_description(f\"Epoch: {epoch}, tloss {loss:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WCFCgt--BvoZ",
    "outputId": "0f3ec12c-b922-449b-c711-42b2a43ea48d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 44.82575225830078\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "# Predict\n",
    "pred = model(x_test)\n",
    "\n",
    "# Measure RMSE error.  RMSE is common for regression.\n",
    "score = np.sqrt(metrics.mean_squared_error(pred.cpu().detach(),y_test.cpu().detach()))\n",
    "print(\"Final score (RMSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "FAEtZKywB0UZ",
    "outputId": "0b532b48-7ffb-4938-d0bd-4a1f9481d665"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Regression chart.\n",
    "def chart_regression(pred, y, sort=True):\n",
    "    t = pd.DataFrame({'pred': pred, 'y': y.flatten()})\n",
    "    if sort:\n",
    "        t.sort_values(by=['y'], inplace=True)\n",
    "    plt.plot(t['y'].tolist(), label='expected')\n",
    "    plt.plot(t['pred'].tolist(), label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Plot the chart\n",
    "chart_regression(pred.flatten().cpu().detach(),y_test.cpu().detach())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "t81_558_class_04_4_backprop.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
