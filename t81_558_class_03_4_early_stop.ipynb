{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZXh7dogJlHH"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/pytorch/t81_558_class_03_4_early_stop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKH1QxMuJlHK"
      },
      "source": [
        "# T81-558: Applications of Deep Neural Networks\n",
        "**Module 3: Introduction to PyTorch**\n",
        "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
        "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwkbs9-gJlHL"
      },
      "source": [
        "# Module 3 Material\n",
        "\n",
        "* Part 3.1: Deep Learning and Neural Network Introduction [[Video]](https://www.youtube.com/watch?v=zYnI4iWRmpc&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_03_1_neural_net.ipynb)\n",
        "* Part 3.2: Introduction to Tensorflow and PyTorch [[Video]](https://www.youtube.com/watch?v=PsE73jk55cE&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_03_2_pytorch.ipynb)\n",
        "* Part 3.3: Saving and Loading a PyTorch Neural Network [[Video]](https://www.youtube.com/watch?v=-9QfbGM1qGw&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_03_3_save_load.ipynb)\n",
        "* **Part 3.4: Early Stopping in PyTorch to Prevent Overfitting** [[Video]](https://www.youtube.com/watch?v=m1LNunuI2fk&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_03_4_early_stop.ipynb)\n",
        "* Part 3.5: Extracting Weights and Manual Calculation [[Video]](https://www.youtube.com/watch?v=7PWgx16kH8s&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_03_5_weights.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovYF1H1ZJlHL"
      },
      "source": [
        "# Google CoLab Instructions\n",
        "\n",
        "The following code ensures that Google CoLab is running and maps Google Drive if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4wO3BiMJlHM",
        "outputId": "726bb114-06c4-4167-c474-f4dc6cb22b42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: using Google CoLab\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWo4ptCdJlHN"
      },
      "source": [
        "# Part 3.4: Early Stopping in Keras to Prevent Overfitting\n",
        "\n",
        "It can be difficult to determine how many epochs to cycle through to train a neural network. Overfitting will occur if you train the neural network for too many epochs, and the neural network will not perform well on new data, despite attaining a good accuracy on the training set. Overfitting occurs when a neural network is trained to the point that it begins to memorize rather than generalize, as demonstrated in Figure 3.OVER. \n",
        "\n",
        "**Figure 3.OVER: Training vs. Validation Error for Overfitting**\n",
        "![Training vs. Validation Error for Overfitting](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_3_training_val.png \"Training vs. Validation Error for Overfitting\")\n",
        "\n",
        "It is important to segment the original dataset into several datasets:\n",
        "\n",
        "* **Training Set**\n",
        "* **Validation Set**\n",
        "* **Holdout Set**\n",
        "\n",
        "You can construct these sets in several different ways. The following programs demonstrate some of these.\n",
        "\n",
        "The first method is a training and validation set. We use the training data to train the neural network until the validation set no longer improves. This attempts to stop at a near-optimal training point. This method will only give accurate \"out of sample\" predictions for the validation set; this is usually 20% of the data. The predictions for the training data will be overly optimistic, as these were the data that we used to train the neural network. Figure 3.VAL demonstrates how we divide the dataset.\n",
        "\n",
        "**Figure 3.VAL: Training with a Validation Set**\n",
        "![Training with a Validation Set](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_1_train_val.png \"Training with a Validation Set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CAezCpVfOFAF"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import copy\n",
        "\n",
        "class EarlyStopping():\n",
        "  def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.restore_best_weights = restore_best_weights\n",
        "    self.best_model = None\n",
        "    self.best_loss = None\n",
        "    self.counter = 0\n",
        "    self.status = \"\"\n",
        "    \n",
        "  def __call__(self, model, val_loss):\n",
        "    if self.best_loss == None:\n",
        "      self.best_loss = val_loss\n",
        "      self.best_model = copy.deepcopy(model)\n",
        "    elif self.best_loss - val_loss > self.min_delta:\n",
        "      self.best_loss = val_loss\n",
        "      self.counter = 0\n",
        "      self.best_model.load_state_dict(model.state_dict())\n",
        "    elif self.best_loss - val_loss < self.min_delta:\n",
        "      self.counter += 1\n",
        "      if self.counter >= self.patience:\n",
        "        self.status = f\"Stopped on {self.counter}\"\n",
        "        if self.restore_best_weights:\n",
        "          model.load_state_dict(self.best_model.state_dict())\n",
        "        return True\n",
        "    self.status = f\"{self.counter}/{self.patience}\"\n",
        "    return False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrsobz8ZJlHO"
      },
      "source": [
        "## Early Stopping with Classification\n",
        "\n",
        "We will now see an example of classification training with early stopping. We will train the neural network until the error no longer improves on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOHorWvTsOzj",
        "outputId": "26a88c8d-9b8a-49c1-cc93-5c08373d1226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Isrrl6hyJlHP",
        "outputId": "7843f262-1c52-4453-e9ba-050f8cb3d29f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1, tloss: 1.1500245332717896, vloss: 0.990146, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 95.87it/s]\n",
            "Epoch: 2, tloss: 0.9323804974555969, vloss: 0.925121, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 159.47it/s]\n",
            "Epoch: 3, tloss: 0.8255293965339661, vloss: 0.798008, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 113.85it/s]\n",
            "Epoch: 4, tloss: 0.8436369895935059, vloss: 0.741427, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 115.03it/s]\n",
            "Epoch: 5, tloss: 0.7527522444725037, vloss: 0.743431, EStop:[1/5]: 100%|██████████| 7/7 [00:00<00:00, 131.52it/s]\n",
            "Epoch: 6, tloss: 0.7536522746086121, vloss: 0.770316, EStop:[2/5]: 100%|██████████| 7/7 [00:00<00:00, 132.60it/s]\n",
            "Epoch: 7, tloss: 0.8679084777832031, vloss: 0.747204, EStop:[3/5]: 100%|██████████| 7/7 [00:00<00:00, 104.86it/s]\n",
            "Epoch: 8, tloss: 0.6780274510383606, vloss: 0.662356, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 133.85it/s]\n",
            "Epoch: 9, tloss: 0.6331034302711487, vloss: 0.621915, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 149.71it/s]\n",
            "Epoch: 10, tloss: 0.6545440554618835, vloss: 0.612198, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 140.39it/s]\n",
            "Epoch: 11, tloss: 0.6620667576789856, vloss: 0.597660, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 134.72it/s]\n",
            "Epoch: 12, tloss: 0.6340699195861816, vloss: 0.652124, EStop:[1/5]: 100%|██████████| 7/7 [00:00<00:00, 183.22it/s]\n",
            "Epoch: 13, tloss: 0.7149586081504822, vloss: 0.590897, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 67.78it/s]\n",
            "Epoch: 14, tloss: 0.5638854503631592, vloss: 0.594686, EStop:[1/5]: 100%|██████████| 7/7 [00:00<00:00, 140.60it/s]\n",
            "Epoch: 15, tloss: 0.5990820527076721, vloss: 0.662996, EStop:[2/5]: 100%|██████████| 7/7 [00:00<00:00, 96.60it/s]\n",
            "Epoch: 16, tloss: 0.5852181911468506, vloss: 0.609063, EStop:[3/5]: 100%|██████████| 7/7 [00:00<00:00, 54.03it/s]\n",
            "Epoch: 17, tloss: 0.6762081384658813, vloss: 0.612187, EStop:[4/5]: 100%|██████████| 7/7 [00:00<00:00, 98.61it/s]\n",
            "Epoch: 18, tloss: 0.6287416815757751, vloss: 0.577207, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 100.30it/s]\n",
            "Epoch: 19, tloss: 0.5853526592254639, vloss: 0.577804, EStop:[1/5]: 100%|██████████| 7/7 [00:00<00:00, 112.89it/s]\n",
            "Epoch: 20, tloss: 0.6112919449806213, vloss: 0.592361, EStop:[2/5]: 100%|██████████| 7/7 [00:00<00:00, 105.05it/s]\n",
            "Epoch: 21, tloss: 0.5575110912322998, vloss: 0.575415, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 99.53it/s]\n",
            "Epoch: 22, tloss: 0.5614225268363953, vloss: 0.575120, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 76.37it/s]\n",
            "Epoch: 23, tloss: 0.6123114228248596, vloss: 0.574724, EStop:[0/5]: 100%|██████████| 7/7 [00:00<00:00, 65.89it/s]\n",
            "Epoch: 24, tloss: 0.6827948689460754, vloss: 0.582095, EStop:[1/5]: 100%|██████████| 7/7 [00:00<00:00, 64.36it/s]\n",
            "Epoch: 25, tloss: 0.5930501222610474, vloss: 0.600552, EStop:[2/5]: 100%|██████████| 7/7 [00:00<00:00, 79.01it/s]\n",
            "Epoch: 26, tloss: 0.5815370082855225, vloss: 0.586718, EStop:[3/5]: 100%|██████████| 7/7 [00:00<00:00, 139.29it/s]\n",
            "Epoch: 27, tloss: 0.5663580298423767, vloss: 0.576089, EStop:[4/5]: 100%|██████████| 7/7 [00:00<00:00, 113.07it/s]\n",
            "Epoch: 28, tloss: 0.5620965957641602, vloss: 0.576314, EStop:[Stopped on 5]: 100%|██████████| 7/7 [00:00<00:00, 117.92it/s]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from sklearn import preprocessing\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import tqdm\n",
        "import time\n",
        "\n",
        "# Define the PyTorch Neural Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, in_count, out_count):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_count, 50)\n",
        "        self.fc2 = nn.Linear(50, 25)\n",
        "        self.fc3 = nn.Linear(25, out_count)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.softmax(self.fc3(x))\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/iris.csv\", \n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "x = df[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\n",
        "y = le.fit_transform(df['species'])\n",
        "species = le.classes_\n",
        "\n",
        "# Split into validation and training sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(    \n",
        "    x, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Numpy to Torch Tensor\n",
        "x_train = torch.Tensor(x_train).float()\n",
        "y_train = torch.Tensor(y_train).long()\n",
        "\n",
        "x_test = torch.Tensor(x_test).float().to(device)\n",
        "y_test = torch.Tensor(y_test).long().to(device)\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "dataset_train = TensorDataset(x_train, y_train)\n",
        "dataloader_train = DataLoader(dataset_train,\\\n",
        "  batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "dataset_test = TensorDataset(x_test, y_test)\n",
        "dataloader_test = DataLoader(dataset_test,\\\n",
        "  batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "# Create model\n",
        "model = Net(x.shape[1],len(species)).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()# cross entropy loss\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "es = EarlyStopping()\n",
        "\n",
        "epoch = 0\n",
        "done = False\n",
        "while epoch<1000 and not done:\n",
        "  epoch += 1\n",
        "  steps = list(enumerate(dataloader_train))\n",
        "  pbar = tqdm.tqdm(steps)\n",
        "  model.train()\n",
        "  for i, (x_batch, y_batch) in pbar:\n",
        "    y_batch_pred = model(x_batch.to(device))\n",
        "    loss = loss_fn(y_batch_pred, y_batch.to(device))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss, current = loss.item(), (i + 1)* len(x_batch)\n",
        "    if i == len(steps)-1:\n",
        "      model.eval()\n",
        "      pred = model(x_test)\n",
        "      vloss = loss_fn(pred, y_test)\n",
        "      if es(model,vloss): done = True\n",
        "      pbar.set_description(f\"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
        "    else:\n",
        "      pbar.set_description(f\"Epoch: {epoch}, tloss {loss:}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model(x_test)\n",
        "vloss = loss_fn(pred, y_test)\n",
        "print(f\"Loss = {vloss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJCDY-FcP41U",
        "outputId": "669ce3c7-fe04-4aba-9368-6910c03919c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss = 0.5747241377830505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATJhTzRjJlHQ"
      },
      "source": [
        "There are a number of parameters that are specified to the **EarlyStopping** object. \n",
        "\n",
        "* **min_delta** This value should be kept small. It simply means the minimum change in error to be registered as an improvement.  Setting it even smaller will not likely have a great deal of impact.\n",
        "* **patience** How long should the training wait for the validation error to improve?  \n",
        "* **restore_best_weights** This should always be set to true.  This restores the weights to the values they were at when the validation set is the highest.  Unless you are manually tracking the weights yourself (we do not use this technique in this course), you should have Keras perform this step for you.\n",
        "\n",
        "As you can see from above, the entire number of requested epochs were not used.  The neural network training stopped once the validation set no longer improved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "A0iNHDxNJlHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e679a794-83d1-489d-ea29-a94653ae48c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9736842105263158\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "pred = model(x_test)\n",
        "_, predict_classes = torch.max(pred, 1)\n",
        "correct = accuracy_score(y_test.cpu(),predict_classes.cpu())\n",
        "print(f\"Accuracy: {correct}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR03ea5QJlHS"
      },
      "source": [
        "## Early Stopping with Regression\n",
        "\n",
        "The following code demonstrates how we can apply early stopping to a regression problem.  The technique is similar to the early stopping for classification code that we just saw."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pTuEcZE4JlHS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "908916f2-cd4a-49a4-ccae-2f8c5af0e190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/19 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "Epoch: 1, tloss 177.66775512695312:  53%|█████▎    | 10/19 [00:00<00:00, 87.00it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "Epoch: 1, tloss: 489.73529052734375, vloss: 679.237183, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 99.29it/s]\n",
            "Epoch: 2, tloss: 100.33761596679688, vloss: 210.123413, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 112.81it/s]\n",
            "Epoch: 3, tloss: 183.453857421875, vloss: 94.120827, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 92.23it/s] \n",
            "Epoch: 4, tloss: 83.3133544921875, vloss: 103.394432, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 99.43it/s] \n",
            "Epoch: 5, tloss: 106.06958770751953, vloss: 91.924225, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 107.99it/s]\n",
            "Epoch: 6, tloss: 102.79740905761719, vloss: 91.185661, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 133.65it/s]\n",
            "Epoch: 7, tloss: 86.6026382446289, vloss: 89.560936, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 72.93it/s]\n",
            "Epoch: 8, tloss: 110.98260498046875, vloss: 89.220161, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 133.81it/s]\n",
            "Epoch: 9, tloss: 132.4139404296875, vloss: 88.915024, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 148.50it/s]\n",
            "Epoch: 10, tloss: 100.97270965576172, vloss: 86.966438, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 161.90it/s]\n",
            "Epoch: 11, tloss: 92.6504898071289, vloss: 83.969261, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 107.77it/s]\n",
            "Epoch: 12, tloss: 50.97971725463867, vloss: 84.754936, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 113.73it/s]\n",
            "Epoch: 13, tloss: 138.0421600341797, vloss: 81.438377, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 70.53it/s]\n",
            "Epoch: 14, tloss: 160.23545837402344, vloss: 82.610420, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 92.19it/s]\n",
            "Epoch: 15, tloss: 79.06620025634766, vloss: 78.131889, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 99.42it/s] \n",
            "Epoch: 16, tloss: 119.15987396240234, vloss: 79.144844, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 114.33it/s]\n",
            "Epoch: 17, tloss: 94.82083129882812, vloss: 75.876167, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 79.17it/s]\n",
            "Epoch: 18, tloss: 44.60140609741211, vloss: 77.382431, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 90.35it/s] \n",
            "Epoch: 19, tloss: 96.90638732910156, vloss: 72.304062, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 110.65it/s]\n",
            "Epoch: 20, tloss: 87.83602142333984, vloss: 71.104355, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 83.34it/s]\n",
            "Epoch: 21, tloss: 33.07319641113281, vloss: 77.825020, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 111.85it/s]\n",
            "Epoch: 22, tloss: 66.97091674804688, vloss: 71.566963, EStop:[2/5]: 100%|██████████| 19/19 [00:00<00:00, 106.97it/s]\n",
            "Epoch: 23, tloss: 66.99259948730469, vloss: 69.699295, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 65.88it/s]\n",
            "Epoch: 24, tloss: 113.96306610107422, vloss: 70.008774, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 91.67it/s] \n",
            "Epoch: 25, tloss: 151.45663452148438, vloss: 81.951103, EStop:[2/5]: 100%|██████████| 19/19 [00:00<00:00, 95.92it/s] \n",
            "Epoch: 26, tloss: 72.63276672363281, vloss: 65.765190, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 96.85it/s] \n",
            "Epoch: 27, tloss: 93.16593170166016, vloss: 69.281586, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 85.45it/s]\n",
            "Epoch: 28, tloss: 95.00636291503906, vloss: 68.742233, EStop:[2/5]: 100%|██████████| 19/19 [00:00<00:00, 93.12it/s]\n",
            "Epoch: 29, tloss: 126.4712142944336, vloss: 87.271088, EStop:[3/5]: 100%|██████████| 19/19 [00:00<00:00, 89.00it/s] \n",
            "Epoch: 30, tloss: 129.0696563720703, vloss: 67.952202, EStop:[4/5]: 100%|██████████| 19/19 [00:00<00:00, 86.72it/s]\n",
            "Epoch: 31, tloss: 95.40279388427734, vloss: 63.695110, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 79.25it/s]\n",
            "Epoch: 32, tloss: 71.45568084716797, vloss: 66.058060, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 78.15it/s]\n",
            "Epoch: 33, tloss: 66.36286163330078, vloss: 64.731552, EStop:[2/5]: 100%|██████████| 19/19 [00:00<00:00, 90.30it/s] \n",
            "Epoch: 34, tloss: 56.10455322265625, vloss: 66.384506, EStop:[3/5]: 100%|██████████| 19/19 [00:00<00:00, 97.61it/s]\n",
            "Epoch: 35, tloss: 65.74290466308594, vloss: 66.703758, EStop:[4/5]: 100%|██████████| 19/19 [00:00<00:00, 95.57it/s]\n",
            "Epoch: 36, tloss: 84.1739273071289, vloss: 62.642475, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 121.90it/s]\n",
            "Epoch: 37, tloss: 112.21161651611328, vloss: 67.154709, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 145.34it/s]\n",
            "Epoch: 38, tloss: 95.08767700195312, vloss: 62.795887, EStop:[2/5]: 100%|██████████| 19/19 [00:00<00:00, 82.48it/s]\n",
            "Epoch: 39, tloss: 95.98158264160156, vloss: 63.837330, EStop:[3/5]: 100%|██████████| 19/19 [00:00<00:00, 100.94it/s]\n",
            "Epoch: 40, tloss: 65.02288818359375, vloss: 61.968468, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 109.21it/s]\n",
            "Epoch: 41, tloss: 73.90129852294922, vloss: 62.011524, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 126.69it/s]\n",
            "Epoch: 42, tloss: 95.55388641357422, vloss: 63.864655, EStop:[2/5]: 100%|██████████| 19/19 [00:00<00:00, 134.49it/s]\n",
            "Epoch: 43, tloss: 88.37999725341797, vloss: 84.714302, EStop:[3/5]: 100%|██████████| 19/19 [00:00<00:00, 95.54it/s]\n",
            "Epoch: 44, tloss: 90.14160919189453, vloss: 64.423370, EStop:[4/5]: 100%|██████████| 19/19 [00:00<00:00, 73.03it/s]\n",
            "Epoch: 45, tloss: 68.76056671142578, vloss: 61.014660, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 83.03it/s]\n",
            "Epoch: 46, tloss: 100.72283172607422, vloss: 65.972282, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 65.70it/s]\n",
            "Epoch: 47, tloss: 85.05509948730469, vloss: 69.076851, EStop:[2/5]: 100%|██████████| 19/19 [00:00<00:00, 85.76it/s]\n",
            "Epoch: 48, tloss: 63.4718132019043, vloss: 60.607498, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 100.20it/s]\n",
            "Epoch: 49, tloss: 66.38623046875, vloss: 78.897728, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 85.49it/s]\n",
            "Epoch: 50, tloss: 32.633140563964844, vloss: 60.490200, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 95.83it/s] \n",
            "Epoch: 51, tloss: 50.53856658935547, vloss: 62.187542, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 125.28it/s]\n",
            "Epoch: 52, tloss: 115.25941467285156, vloss: 60.234840, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 105.04it/s]\n",
            "Epoch: 53, tloss: 70.4327392578125, vloss: 75.528397, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 123.16it/s]\n",
            "Epoch: 54, tloss: 75.64313507080078, vloss: 71.500282, EStop:[2/5]: 100%|██████████| 19/19 [00:00<00:00, 116.05it/s]\n",
            "Epoch: 55, tloss: 34.49662780761719, vloss: 62.627678, EStop:[3/5]: 100%|██████████| 19/19 [00:00<00:00, 114.73it/s]\n",
            "Epoch: 56, tloss: 63.2104377746582, vloss: 60.352379, EStop:[4/5]: 100%|██████████| 19/19 [00:00<00:00, 144.57it/s]\n",
            "Epoch: 57, tloss: 57.884788513183594, vloss: 59.828400, EStop:[0/5]: 100%|██████████| 19/19 [00:00<00:00, 89.20it/s]\n",
            "Epoch: 58, tloss: 42.74074935913086, vloss: 84.952637, EStop:[1/5]: 100%|██████████| 19/19 [00:00<00:00, 99.04it/s]\n",
            "Epoch: 59, tloss: 50.52678298950195, vloss: 61.150635, EStop:[2/5]: 100%|██████████| 19/19 [00:00<00:00, 81.95it/s] \n",
            "Epoch: 60, tloss: 29.41653823852539, vloss: 64.532784, EStop:[3/5]: 100%|██████████| 19/19 [00:00<00:00, 100.07it/s]\n",
            "Epoch: 61, tloss: 55.98590850830078, vloss: 59.925560, EStop:[4/5]: 100%|██████████| 19/19 [00:00<00:00, 90.81it/s]\n",
            "Epoch: 62, tloss: 48.57834243774414, vloss: 67.305519, EStop:[Stopped on 5]: 100%|██████████| 19/19 [00:00<00:00, 98.51it/s]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from sklearn import preprocessing\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import tqdm\n",
        "import time\n",
        "\n",
        "# Define the PyTorch Neural Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, in_count, out_count):\n",
        "        super(Net, self).__init__()\n",
        "        # We must define each of the layers.\n",
        "        self.fc1 = nn.Linear(in_count, 50)\n",
        "        self.fc2 = nn.Linear(50, 25)\n",
        "        self.fc3 = nn.Linear(25, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # In the forward pass, we must calculate all of the layers we \n",
        "        # previously defined.\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Read the MPG dataset.\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", \n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "cars = df['name']\n",
        "\n",
        "# Handle missing value\n",
        "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
        "\n",
        "# Pandas to Numpy\n",
        "x = df[['cylinders', 'displacement', 'horsepower', 'weight',\n",
        "       'acceleration', 'year', 'origin']].values\n",
        "y = df['mpg'].values # regression\n",
        "\n",
        "# Numpy to PyTorch\n",
        "x = torch.Tensor(x).float()\n",
        "y = torch.Tensor(y).float()\n",
        "\n",
        "# Split into validation and training sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(    \n",
        "    x, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Numpy to Torch Tensor\n",
        "x_train = torch.Tensor(x_train).float()\n",
        "y_train = torch.Tensor(y_train).float()\n",
        "\n",
        "x_test = torch.Tensor(x_test).float().to(device)\n",
        "y_test = torch.Tensor(y_test).float().to(device)\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "dataset_train = TensorDataset(x_train, y_train)\n",
        "dataloader_train = DataLoader(dataset_train,\\\n",
        "  batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "dataset_test = TensorDataset(x_test, y_test)\n",
        "dataloader_test = DataLoader(dataset_test,\\\n",
        "  batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "# Create model\n",
        "model = Net(x.shape[1],1).to(device)\n",
        "\n",
        "# Define the loss function for regression\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "es = EarlyStopping()\n",
        "\n",
        "epoch = 0\n",
        "done = False\n",
        "while epoch<1000 and not done:\n",
        "  epoch += 1\n",
        "  steps = list(enumerate(dataloader_train))\n",
        "  pbar = tqdm.tqdm(steps)\n",
        "  model.train()\n",
        "  for i, (x_batch, y_batch) in pbar:\n",
        "    y_batch_pred = model(x_batch.to(device))\n",
        "    loss = loss_fn(y_batch_pred, y_batch.to(device))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss, current = loss.item(), (i + 1)* len(x_batch)\n",
        "    if i == len(steps)-1:\n",
        "      model.eval()\n",
        "      pred = model(x_test)\n",
        "      vloss = loss_fn(pred, y_test)\n",
        "      if es(model,vloss): done = True\n",
        "      pbar.set_description(f\"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
        "    else:\n",
        "      pbar.set_description(f\"Epoch: {epoch}, tloss {loss:}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjvaHmp5JlHS"
      },
      "source": [
        "Finally, we evaluate the error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0bvqiX-AJlHS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63346a47-34dd-4382-f8e3-08ccd323f2f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final score (RMSE): 8.163707733154297\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Measure RMSE error.  RMSE is common for regression.\n",
        "pred = model(x_test)\n",
        "score = np.sqrt(metrics.mean_squared_error(pred.cpu().detach(),y_test.cpu().detach()))\n",
        "print(f\"Final score (RMSE): {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYi-h2LNXqoZ",
        "outputId": "0105b1fd-ec52-432a-ddb6-ff02d7e25a70"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([33.0000, 28.0000, 19.0000, 13.0000, 14.0000, 27.0000, 24.0000, 13.0000,\n",
              "        17.0000, 21.0000, 15.0000, 38.0000, 26.0000, 15.0000, 25.0000, 12.0000,\n",
              "        31.0000, 17.0000, 16.0000, 31.0000, 22.0000, 22.0000, 22.0000, 33.5000,\n",
              "        18.0000, 44.0000, 26.0000, 24.5000, 18.1000, 12.0000, 27.0000, 36.0000,\n",
              "        23.0000, 24.0000, 37.2000, 16.0000, 21.0000, 19.2000, 16.0000, 29.0000,\n",
              "        26.8000, 27.0000, 18.0000, 10.0000, 23.0000, 36.0000, 26.0000, 25.0000,\n",
              "        25.0000, 25.0000, 22.0000, 34.1000, 32.4000, 13.0000, 23.5000, 14.0000,\n",
              "        18.5000, 29.8000, 28.0000, 19.0000, 11.0000, 33.0000, 23.0000, 21.0000,\n",
              "        23.0000, 25.0000, 23.8000, 34.4000, 24.5000, 13.0000, 34.7000, 14.0000,\n",
              "        15.0000, 18.0000, 25.0000, 19.9000, 17.5000, 28.0000, 29.0000, 17.0000,\n",
              "        16.0000, 27.0000, 37.0000, 36.1000, 23.0000, 14.0000, 32.8000, 29.9000,\n",
              "        20.0000, 12.0000, 15.5000, 23.7000, 24.0000, 36.0000, 19.0000, 38.0000,\n",
              "        29.0000, 21.5000, 27.9000, 14.0000], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqUztPo3JlHT"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "name": "New t81_558_class_03_4_early_stop.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}